<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Deep Learning Optimization" /><meta property="og:locale" content="en" /><meta name="description" content="딥러닝에서의 최적화(Optimization)란 무엇일까?" /><meta property="og:description" content="딥러닝에서의 최적화(Optimization)란 무엇일까?" /><link rel="canonical" href="https://snmhz.github.io/posts/dl-optimization/" /><meta property="og:url" content="https://snmhz.github.io/posts/dl-optimization/" /><meta property="og:site_name" content="Archive" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-02-08T03:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Deep Learning Optimization" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-06-06T20:32:37+09:00","datePublished":"2022-02-08T03:00:00+09:00","description":"딥러닝에서의 최적화(Optimization)란 무엇일까?","headline":"Deep Learning Optimization","mainEntityOfPage":{"@type":"WebPage","@id":"https://snmhz.github.io/posts/dl-optimization/"},"url":"https://snmhz.github.io/posts/dl-optimization/"}</script><title>Deep Learning Optimization | Archive</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Archive"><meta name="application-name" content="Archive"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.20.1/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener('change', () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.notify(); } /* flipMode() */ } /* ModeToggle */ const modeToggle = new ModeToggle(); </script><body data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/image/bio-photo.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title"> <a href="/">Archive</a></div><div class="site-subtitle font-italic">Hi, there</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/SNMHZ" aria-label="github" target="_blank" rel="noopener noreferrer"> <i class="fab fa-github"></i> </a> <a href="javascript:location.href = 'mailto:' + ['snmhz325','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Deep Learning Optimization</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>Deep Learning Optimization</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1644256800" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Feb 8, 2022 </em> </span> <span> Updated <em class="" data-ts="1749209557" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jun 6, 2025 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/SNMHZ">SNMHZ</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2894 words"> <em>16 min</em> read</span></div></div></div><div class="post-content"><p>딥러닝에서의 최적화(Optimization)란 무엇일까?</p><p>단순하게 생각하면<br /> 랜덤으로 초기화된 모델의 weight들을<br /> Gradient Descent를 통해 cost를 최소화하는 방향으로 변화시키는 것<br /> 이라고 답변할 수 있을 것이다.</p><p>하지만, 그 과정에서 고민하고 확인해야 할 것이 무엇이 있는지, 이를 위해 체크하면 좋을 것들에 대해 정리하고자 한다.</p><h2 id="일반화generalization에-대하여"><span class="mr-2">일반화(Generalization)에 대하여</span><a href="#일반화generalization에-대하여" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>최적화의 목표 중 하나는, <code class="language-plaintext highlighter-rouge">일반화 성능</code>을 높이는 것이라 할 수 있다.</p><h3 id="일반화-성능이란-무엇인가"><span class="mr-2">일반화 성능이란 무엇인가?</span><a href="#일반화-성능이란-무엇인가" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><blockquote><p>일반화 성능을 높이면 무조건 좋은건가? <br /> 일반화란 어떤 의미일까?</p></blockquote><p>학습을 시키면 모델의 weight들이 training set의 cost를 최소화하는 방향으로 업데이트 된다.<br /> 계속 학습을 시키게 되면 점점 더 잘 맞추게 된다.<br /> 단, 일정 수준을 넘어버리면 training set에 과적합되어 test set에 대해선 잘 맞추지 못하는 모습을 보인다. <a href="/image/boostcamp/dl-optimization/under-over-fitting.png" class="popup img-link "><img data-src="/image/boostcamp/dl-optimization/under-over-fitting.png" width="100%" class="lazyload" data-proofer-ignore></a><br /> <sup><a href="https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html">reference</a>. 모두에게 너무나 익숙한 그 그림</sup></p><p>학습 데이터에 너무 맞추는 것도 아니고, 너무 못 맞추는 것도 아니고.<br /> 그 중간 어딘가가 가장 좋은(Balanced) 지점이라 할 수 있다.</p><p>하지만 이는 너무 이론적인 말이다.<br /> 실제 문제에서 항상 똑같이 적용할 수 있다고 할 수는 없다.</p><p>실제 풀고자 하는 문제의 타겟은<br /> 저 급격하게 변하는 모양새의 오버피팅된 모습일 수도 있고, <br /> 아예 다른 형태일 수도 있다.</p><p>우리가 가정하고 있는 것은 <br /> 데이터가 학습하고자 하는 어떤 목적에서 발생된<br /> 구조적인 형태를 가지고 있을 것이라고 기대 뿐이다.</p><p>이 부분은 디테일한 분석을 통해 데이터에 대해 깊게 이해하고,<br /> 적절하게 문제를 정의하는 것이 가장 중요하다고 생각된다.</p><h3 id="일반화된-지점을-어떻게-확인할까"><span class="mr-2">일반화된 지점을 어떻게 확인할까?</span><a href="#일반화된-지점을-어떻게-확인할까" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="/image/boostcamp/dl-optimization/generalization-gap.png" class="popup img-link "><img data-src="/image/boostcamp/dl-optimization/generalization-gap.png" width="90%" class="lazyload" data-proofer-ignore></a><br /> test error와 training error의 차이를 통해 얻는 <code class="language-plaintext highlighter-rouge">일반화 갭의 상태를 보고 결정</code>할 수 있다.<br /> 단, 이를 통해서는 <code class="language-plaintext highlighter-rouge">이 모델의 성능이 학습 성능이랑 비슷할 것</code>이란 사실만 알 수 있다.<br /> 학습 데이터에 대해 성능이 좋지 않으면<sub>(학습을 덜 했거나, 데이터에 노이즈가 너무 많이 껴 있거나, 데이터와 맞지 않는 모델을 사용했거나 등등..)</sub>, <code class="language-plaintext highlighter-rouge">일반화 갭이 작다고 해서 잘 학습되었다고 할 수는 없다</code>.</p><ul><li><code class="language-plaintext highlighter-rouge">일반화 성능이 좋다</code> != <code class="language-plaintext highlighter-rouge">테스트 데이터 성능이 좋을 것이다</code><li><code class="language-plaintext highlighter-rouge">일반화 성능이 좋다</code> == <code class="language-plaintext highlighter-rouge">테스트 데이터 성능이 학습 데이터 성능과 비슷할 것이다</code></ul><p><br /><br /></p><h2 id="모델의-성능에-대한-bias와-variance"><span class="mr-2">모델의 성능에 대한 Bias와 Variance</span><a href="#모델의-성능에-대한-bias와-variance" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>모델의 파라미터(weight, bias 등)를 말하는 것이 아님에 유의<br /> 사격 시 탄착군과 비슷한 개념으로 생각할 수 있다.</p><p><a href="/image/boostcamp/dl-optimization/high-low-bias-variance.png" class="popup img-link "><img data-src="/image/boostcamp/dl-optimization/high-low-bias-variance.png" width="50%" class="lazyload" data-proofer-ignore></a><br /></p><h3 id="모델의-성능에-대한-bias"><span class="mr-2">모델의 성능에 대한 Bias</span><a href="#모델의-성능에-대한-bias" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><blockquote><p>비슷한 입력에 대해서 True Target에 얼마나 접근하는가.</p></blockquote><ul><li>bias가 낮다<ul><li>출력이 많이 분산 되더라도 평균적으로 True Target에 접근하는 경우</ul><li>bias가 높다<ul><li>True Target에 대해 평균적으로 많이 벗어나는 경우</ul></ul><h3 id="모델의-성능에-대한-variance"><span class="mr-2">모델의 성능에 대한 Variance</span><a href="#모델의-성능에-대한-variance" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><blockquote><p>비슷한 입력을 넣었을 때 출력이 얼마나 일관적으로 나오는가</p></blockquote><ul><li>variance가 낮다<ul><li>간단한 모델이 이런 경우가 많을 것이다. 비슷한 입력에 대해 둔감한 변화를 보인다.</ul><li>variance가 높다<ul><li>비슷한 입력에 출력이 많이 달라진다. overfitting이 생길 가능성이 높아진다.</ul></ul><h3 id="bias-and-variance-trade-off"><span class="mr-2">Bias and Variance Trade-off</span><a href="#bias-and-variance-trade-off" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="/image/boostcamp/dl-optimization/cost-bias-variance-noise.png" class="popup img-link "><img data-src="/image/boostcamp/dl-optimization/cost-bias-variance-noise.png" width="90%" class="lazyload" data-proofer-ignore></a><br /> 노이즈가 학습 데이터에 노이즈가 껴 있다고 가정했을 때,<br /> cost는 bias, variance, noise 3가지 파트로 이루어져 있다.</p><p>데이터의 cost를 minimize하는 것은 사실 각각을 minimize 하는 것이 아니다.<br /></p><p>따라서, 하나가 줄어들면 하나가 커질 수 밖에 없고,<br /> 각 파트는 trade-off의 관계에 있다.<br /></p><p>이는 모델의 성능에 대한 이론적 한계(fundamental limit)가 된다.</p><p><br /><br /></p><h2 id="gradient-descent에-대하여"><span class="mr-2">Gradient Descent에 대하여</span><a href="#gradient-descent에-대하여" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="학습시-활용하는-데이터-수배치-사이즈에-따른-분류"><span class="mr-2">학습시 활용하는 데이터 수(배치 사이즈)에 따른 분류</span><a href="#학습시-활용하는-데이터-수배치-사이즈에-따른-분류" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Stocastic Gradient Descent<ul><li>한 개 sample씩만 활용</ul><li>Mini-batch Gradient Descent<ul><li>일부 sample을 모아 data를 subset으로 만들어 활용</ul><li>Batch Gradient Descent<ul><li>전체 데이터를 한번에 활용</ul></ul><h3 id="배치-사이즈를-얼마로-잡아야-할까"><span class="mr-2">배치 사이즈를 얼마로 잡아야 할까?</span><a href="#배치-사이즈를-얼마로-잡아야-할까" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>1개를 쓰면 너무 오래걸리고, <br /> 너무 많이 넣으면 GPU 메모리가 터지고,<br /> 적절한 수를 찾아야 한다. <a href="/image/boostcamp/dl-optimization/flat-minimum-advantage.png" class="popup img-link "><img data-src="/image/boostcamp/dl-optimization/flat-minimum-advantage.png" width="100%" class="lazyload" data-proofer-ignore></a><br /> <sup><a href="https://arxiv.org/abs/1609.04836">논문: On Large-Batch Training for Deep Learning</a></sup></p><ul><li>라지 배치사이즈를 사용하면 sharp minimum에 도착한다.<li>sharp minimum에서는 testing function에서 조금만 멀어져도 잘 동작하지 않을 수 있다.(위 이미지의 보라색 선 참고)<li>flat minimum에 도착하면 testing function에서 조금 멀어져도 괜찮은 성능을 기대할 수 있다.<li>flat minimum에 도착하면 일반화 성능이 높아진다.<li>sharp minimum보다는 flat minimum에 도착하는 것이 더 좋다.<li>배치 사이즈를 작게 쓰는게 일반적으로 좋다.</ul><h3 id="gradient-desent-methods"><span class="mr-2">Gradient Desent Methods</span><a href="#gradient-desent-methods" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>똑같이 Gradient Information만 이용해서<br /> 어떻게 더 좋은 성능, 혹은 더 빠른 학습을 시킬 수 있을까? 에 대한 고민</p><p>자동으로 미분을 해 주는 딥러닝 프레임워크의 핵심으로,<br /> Optimizer로 구현되어 있고 적절한 것을 골라 활용할 수 있다.</p><p>각각이 왜 제안이 되었고, 어떤 성질이 있는지를 알아두면 좋다. <a href="/image/boostcamp/dl-optimization/optimizers-concept.png" class="popup img-link "><img data-src="/image/boostcamp/dl-optimization/optimizers-concept.png" width="100%" class="lazyload" data-proofer-ignore></a><br /> <sup><a href="https://www.slideshare.net/yongho/ss-79607172">reference</a></sup></p><ul><li>(Stocastic) Gradient Descent<ul><li>가장 기본적인 GD를 활용하는 방법.<li>Gradient를 구해서 learning rate만큼 빼준다.<li>lr을 적절히 잡아주는게 매우 어렵다.</ul><li>Momentum<ul><li>관성<li>이전 배치에서 어느 방향으로 흘렀는지에 대한 정보를 활용하자.<li>한번 흘렀으면, 다음번에 조금 다르게 흘러도 이쪽 방향으로 흐르던 정보를 이어가자.<li>momentum과 현재 Gradient를 합친 Accumulation Gradient를 사용<li>한번 흐른 Gradient를 유지시켜줘서 Gradient가 왔다갔다해도 잘 훈련되도록 도와준다.</ul><li>NAG(Nestrov Accelerated Gradient)<ul><li>Gradient를 계산할 때 Lookahead Gradient를 계산한다.<li>현재 자리에서 한번 가 보고 간 자리에서 계산한 것으로 Accumulation.<li>위의 방법들은 local minima에 왔다갔다 하며 닿지 못하는 모습을 보일 수 있는데, 이를 봉우리에 닿도록 도와줄 수 있다.</ul><li>Adagrad (Adaptive grad)<ul><li>파라미터가 지금까지 얼마나 변해왔는지를 확인한다.<li>많이 변한 파라미터는 적게 변화시키고, 적게 변한 파라미터는 많이 변화시킨다.<li>adaptive lr을 활용하게 된다.<li>G가 계속 커지기 때문에 G가 무한대로 갈 수록 점차 학습이 멈추게 되는 문제가 있다. <a href="/image/boostcamp/dl-optimization/adagrad.png" class="popup img-link "><img data-src="/image/boostcamp/dl-optimization/adagrad.png" width="80%" class="lazyload" data-proofer-ignore></a><br /></ul><li>Adadelta<ul><li>G가 계속 커지는 현상을 막겠다.<li>타임스탬프 t를 윈도우 사이즈 만큼의 그래디언트 변화를 보겠다<li>이전 t개 동안의 G를 들고 있어야 된다.<li>파라미터가 커지면 힘들다. (파라미터 수 * t의 공간 필요)<li>lr이 없다.</ul><li>RMSprop<ul><li>논문을 통해 제안된 것은 아니고, Geoff Hinton이 강의에서 이러니까 잘 되더라 한게 레퍼런스(…)<li>Adagrad에서 G를 구할 때 그냥 gradient square를 더하는 것이 아니라, exponential moving average를 더해 준다.<li>stepsize(η)를 사용한다. <a href="/image/boostcamp/dl-optimization/rmsprop.png" class="popup img-link "><img data-src="/image/boostcamp/dl-optimization/rmsprop.png" width="80%" class="lazyload" data-proofer-ignore></a><br /></ul><li>Adam (Adaptive Moment Estimation)<ul><li>일반적으로 가장 무난하게 사용.<li>RMSprop을 함과 동시에, 모멘텀을 같이 활용<li>β1 : 모멘텀을 얼마나 유지시키는지<li>β2 : gradient squares에 대한 EMA 정보<li>η : learning rate<li>ε : div by zero를 막기 위한 파라미터지만, 이 값을 잘 바꿔주는것도 실질적으론 중요하다 <a href="/image/boostcamp/dl-optimization/adam.png" class="popup img-link "><img data-src="/image/boostcamp/dl-optimization/adam.png" width="80%" class="lazyload" data-proofer-ignore></a><br /></ul></ul><h3 id="regularization-규제"><span class="mr-2">Regularization. 규제</span><a href="#regularization-규제" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>학습을 방해하는게 목적이다.<br /></p><p>학습을 방해함으로써 얻는 이점은<br /> 학습 데이터에서 뿐만 아니라<br /> 테스트 데이터에 대해서도 잘 동작하도록 하기 위함이다.</p><ul><li>Early stopping<ul><li>loss 상황을 계속 보면서 일찍 학습을 멈추자.<li>단, 학습을 멈출 때 test data를 활용하면 cheating이다.<li>보통 validation error를 이용.</ul><li>Parameter norm penalty<ul><li>네트워크 파라미터가 너무 커지지 않도록 한다.<li>이왕이면 네트워크 weight가 작은 것이 좋다.<li>뉴럴넷이 만드는 함수의 공간(function space)을 최대한 부드러운 형태로 만들자.<ul><li>weight가 작으면 function space가 부드러워진다.<li>부드러운 함수일 수록 일반화 성능이 높을 것이다..! 라는 기대</ul></ul><li>Data augmentation<ul><li>뉴럴넷에서 가장 중요한 것 중 하나.<li>데이터가 적으면, DL보다 일반적인 ML방법론이 더 좋다.<li>데이터가 많아지면, 많은 데이터에 대한 표현력이 ML방법론에선 부족하다.<ul><li>따라서 DL방법론의 성능이 더 좋아진다.</ul><li>단, 데이터를 변화시킴에도 정답이 변화하지 않는 수준에서 변화를 시킨다.</ul><li>Noise robustness<ul><li><del>사실 왜 잘되는지 아직 의문이 있긴 하다…</del>ㅋㅋ(완전히 해석되지 않았다)<li>입력 데이터에 noise를 넣는 것은 Data augmentation의 일부로 생각할 수도 있다.<li>학습시킬 때 노이즈를 웨이트에 넣어줘도(weight를 흔들어도) 좋을 수 있다…</ul><li>Label Smoothing<ul><li>다른 Label의 샘플을 뽑아서 데이터와 라벨을 섞어준다.<li>왜 잘 될까,,,?<ul><li>결국, 데이터들이 있는 공간 속에서 Decision Boundary를 찾는게 목표.<li>이 경계를 부드럽게 만들어 주는 효과.</ul><li>ex. mixup, cutmix<br /> <a href="/image/boostcamp/dl-optimization/cutmix-capture.png" class="popup img-link "><img data-src="/image/boostcamp/dl-optimization/cutmix-capture.png" width="80%" class="lazyload" data-proofer-ignore></a><br /> <sup><a href="https://arxiv.org/abs/1905.04899">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</a></sup></ul><li>Dropout<ul><li>뉴럴넷의 weight를 랜덤하게 0으로 바꿔준다.<li>각각의 뉴런들이 좀 더 robust한 feature를 잡을 수 있다라고 해석을 한다.. (수학적으로 증명된 것은 아님.)<li>서로 다른 N개의 신경망을 앙상블하는 형태라 표현하기도 한다.<li>일반적으로 쓰면 성능이 많이 올라가는 효과를 보인다.</ul><li>Batch normalization<ul><li><del>논란이 참 많다..</del>ㅋㅋ(완전히 해석되지 않았다)<li>내가 적용하고자 하는 BN 레이어의 통계량을 정규화.<li>레이어 단 입력의 각각의 값들에 대하여 평균이 0인 정규분포로 만들어버린다.<li>대부분의 경우 성능이 많이 올라간다… 성능을 올리는 것이 목표라면 활용하는게 좋다.<li><a href="https://velog.io/@choiking10/Batch-Normalization%EC%97%90-%EB%8C%80%ED%95%B4%EC%84%9C-%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90">이 글</a>에 매우 잘 정리되어 있다.</ul></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/naver-boostcamp-ai-tech/'>NAVER BoostCamp AI Tech</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/naver-boostcamp-ai-tech/" class="post-tag no-text-decoration" >NAVER BoostCamp AI Tech</a> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >Deep Learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Deep%20Learning%20Optimization%20-%20Archive&url=https%3A%2F%2Fsnmhz.github.io%2Fposts%2Fdl-optimization%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Deep%20Learning%20Optimization%20-%20Archive&u=https%3A%2F%2Fsnmhz.github.io%2Fposts%2Fdl-optimization%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fsnmhz.github.io%2Fposts%2Fdl-optimization%2F&text=Deep%20Learning%20Optimization%20-%20Archive" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/airflow-dag-parsing-optimization/">Airflow DAG 파싱 주기 설정 시 주의할 점과 최적화 전략</a><li><a href="/posts/embedding-and-latent-vector/">Embedding과 Latent Vector, 헷갈리는 개념 정리</a><li><a href="/posts/fft-and-latent-vector/">FFT 압축 결과는 왜 Latent Vector가 아닐까?</a><li><a href="/posts/py310-to-py311-str-enum-changes/">Python 3.10에서 3.11로 마이그레이션 시 str, Enum 문자열 출력 동작 변경과 대응 전략</a><li><a href="/posts/python-keeps-you-on-your-toes/">파이썬 개발자가 가져야 할 자세</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/naver-boostcamp-ai-tech/">NAVER BoostCamp AI Tech</a> <a class="post-tag" href="/tags/deep-learning/">Deep Learning</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/%ED%9A%8C%EA%B3%A0/">회고</a> <a class="post-tag" href="/tags/airflow/">Airflow</a> <a class="post-tag" href="/tags/boj/">boj</a> <a class="post-tag" href="/tags/data-compression/">Data Compression</a> <a class="post-tag" href="/tags/embedding/">embedding</a> <a class="post-tag" href="/tags/enum/">Enum</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc"></nav></div><script src="https://cdn.jsdelivr.net/npm/tocbot@4.20.1/dist/tocbot.min.js"></script></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4 mt-5"><div id="related-posts" class="mb-2 mb-sm-4"><h3 class="pt-2 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/dl-basics/"><div class="card-body"> <em class="small" data-ts="1644170400" data-df="ll" > Feb 7, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Deep Learning 기본</h3><div class="text-muted small"><p> 딥러닝의 중요 요소 학습을 시킬 데이터(Data) 학습할 모델(Model) 학습시키기 위한 손실 함수(Loss) 손실을 최소화하는 최적화 알고리즘(Optimization Algorithm) 여러 차원의 벡터 공간과 선형 변환 세상이 선형으로만 이루어져있지 않다. 1차원에서 1차원으로 가는 변환만을 찾지 않는다. N차원에...</p></div></div></a></div><div class="card"> <a href="/posts/embedding-and-latent-vector/"><div class="card-body"> <em class="small" data-ts="1746457200" data-df="ll" > May 6, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Embedding과 Latent Vector, 헷갈리는 개념 정리</h3><div class="text-muted small"><p> 서론 머신러닝과 딥러닝에서 embedding, embedding vector, latent vector는 종종 혼용되거나 모호하게 사용되는 개념입니다. 이 세 용어는 서로 연관되어 있지만 엄밀히는 다른 의미를 가지며, 혼동할 경우 모델 해석이나 구현에 실수가 발생할 수 있습니다. 이 글에서는 각 개념의 정의와 차이를 명확히 정리해보겠습니다. ...</p></div></div></a></div><div class="card"> <a href="/posts/fft-and-latent-vector/"><div class="card-body"> <em class="small" data-ts="1746543600" data-df="ll" > May 7, 2025 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>FFT 압축 결과는 왜 Latent Vector가 아닐까?</h3><div class="text-muted small"><p> 서론 FFT(Fast Fourier Transform) 기반 압축과 Latent Vector는 모두 데이터를 효율적으로 표현한다는 공통점이 있지만, 그 메커니즘과 목적에서 근본적인 차이가 있습니다. 마치, FFT 압축을 수행한 이미지를 쭉 나열하면 Latent Vector로써 기능할 수 있을 것 같은 착각을 불러옵니다. 이 글에서는 두 방법의 ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/dl-basics/" class="btn btn-outline-primary" prompt="Older"><p>Deep Learning 기본</p></a> <a href="/posts/boj-1987/" class="btn btn-outline-primary" prompt="Newer"><p>백준 1987 - 알파벳</p></a></div></div></div></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/naver-boostcamp-ai-tech/">NAVER BoostCamp AI Tech</a> <a class="post-tag" href="/tags/deep-learning/">Deep Learning</a> <a class="post-tag" href="/tags/python/">Python</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/%ED%9A%8C%EA%B3%A0/">회고</a> <a class="post-tag" href="/tags/airflow/">Airflow</a> <a class="post-tag" href="/tags/boj/">boj</a> <a class="post-tag" href="/tags/data-compression/">Data Compression</a> <a class="post-tag" href="/tags/embedding/">embedding</a> <a class="post-tag" href="/tags/enum/">Enum</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><footer><div class="container pl-lg-4 pr-lg-4"><div class="d-flex justify-content-between align-items-center text-muted ml-md-3 mr-md-3"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/SNMHZ">SNMHZ</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0">Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></div></footer><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lazysizes@5.3.2/lazysizes.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-MY8205FV4G"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-MY8205FV4G'); }); </script>
