[ { "title": "Pydantic RootModel의 설계 의도와 v2에서의 올바른 타입 매핑", "url": "/posts/pydantic-rootmodel-design-philosophy/", "categories": "Tech, Python", "tags": "Pydantic, Python, 타입검증", "date": "2025-06-19 00:00:00 +0900", "snippet": "서론Pydantic은 Python에서 데이터 검증과 설정 관리를 위한 핵심 라이브러리로 자리잡았습니다. 그 중에서도 RootModel(또는 v1의 __root__ 필드)은 특별한 용도로 설계된 기능입니다.하지만 많은 개발자들이 이 기능을 자동 타입 매핑 용도로 활용하면서, v2에서의 변경사항과 함께 혼란이 생겨났습니다.이 글에서는 RootModel의 ...", "content": "서론Pydantic은 Python에서 데이터 검증과 설정 관리를 위한 핵심 라이브러리로 자리잡았습니다. 그 중에서도 RootModel(또는 v1의 __root__ 필드)은 특별한 용도로 설계된 기능입니다.하지만 많은 개발자들이 이 기능을 자동 타입 매핑 용도로 활용하면서, v2에서의 변경사항과 함께 혼란이 생겨났습니다.이 글에서는 RootModel의 본래 설계 의도부터 실제 사용 패턴, 그리고 v2에서의 변화와 권장되는 구현 방법까지 체계적으로 다루어보겠습니다.목차 1. RootModel의 원래 설계 의도 2. 자동 타입 매핑의 부수적 활용 3. Pydantic v2에서의 변화 4. v2에서 권장되는 자동 타입 매핑 방법 5. v1/v2 호환 코드 작성 전략 6. 구현 방향성과 모범 사례 7. 결론 및 권장사항1. RootModel의 원래 설계 의도핵심 목적 - 단일 값 래핑Pydantic의 RootModel은 “모델 전체가 하나의 값만을 가질 때 그 값을 검증하고 감싸는 것”이 본래 목적입니다. 일반적인 BaseModel이 여러 필드를 가진 구조화된 데이터를 다루는 반면, RootModel은 단순한 리스트, 딕셔너리, 또는 단일 객체 전체를 하나의 “루트 값”으로 취급합니다.Pydantic v1에서의 구현v1에서는 __root__ 필드를 통해 이를 구현했습니다.from pydantic import BaseModelfrom typing import List, Dictclass Pets(BaseModel): __root__: List[str]class PetsByName(BaseModel): __root__: Dict[str, str]# 사용 예시pets = Pets.parse_obj(['dog', 'cat'])print(pets.__root__) # ['dog', 'cat']이 방식의 핵심은 입력 데이터 전체가 곧 모델의 값이라는 점입니다. 복잡한 필드 구조 없이 단순한 컬렉션이나 값을 Pydantic의 검증 시스템 내에서 다룰 수 있게 해줍니다.2. 자동 타입 매핑의 부수적 활용Union과 Discriminator의 조합v1에서 개발자들은 __root__ 필드에 Union 타입을 적용하여 자동 타입 매핑을 구현했습니다. 이것은 본래 의도된 용법은 아니었지만, 실용적인 해결책으로 널리 사용되었습니다.from typing import Union, Literalfrom pydantic import BaseModel, Fieldclass MySchema1(BaseModel): type: Literal['schema1'] a: int b: intclass MySchema2(BaseModel): type: Literal['schema2'] a: int c: intclass RootModel(BaseModel): __root__: Union[MySchema1, MySchema2] = Field(discriminator='type')이 패턴은 입력 데이터의 특정 필드(discriminator) 값에 따라 자동으로 적절한 스키마로 매핑하는 기능을 제공했습니다.왜 이런 활용이 가능했는가v1의 __root__ 구조는 충분히 유연해서 Union 타입과 discriminator를 함께 사용할 수 있었습니다. 이는 설계상 의도된 것은 아니었지만, 실제로는 매우 유용한 패턴으로 자리잡았습니다.개발자들은 이를 통해 하나의 API 엔드포인트에서 여러 다른 형태의 데이터를 받아 자동으로 적절한 모델로 파싱하는 기능을 구현할 수 있었습니다.3. Pydantic v2에서의 변화RootModel의 재설계v2에서는 __root__ 필드가 완전히 제거되고 RootModel 클래스로 대체되었습니다. 이는 단순한 API 변경이 아니라 본질적 목적에 맞는 재설계였습니다.from pydantic import RootModelfrom typing import List# v2 방식class Pets(RootModel[List[str]]): passpets = Pets.model_validate(['dog', 'cat'])print(pets.root) # ['dog', 'cat']자동 타입 매핑 지원의 제한v2의 RootModel은 단일 값 래핑이라는 본래 목적에 더욱 집중하도록 설계되었습니다. 이 과정에서 v1에서 가능했던 Union + discriminator 조합이 공식적으로 지원되지 않게 되었습니다.실제로 GitHub 이슈 #9830에서 확인할 수 있듯이, RootModel에 discriminator를 적용하면 TypeError가 발생하거나 예상과 다르게 동작하는 알려진 버그가 존재합니다.마이그레이션의 어려움이러한 변화로 인해 v1에서 DictionaryInspectorClass.parse_obj(_rows).__root__ 같은 패턴을 사용하던 코드들은 v2에서 DictionaryInspectorClass.model_validate(_rows).root로 변경해야 하지만, 동시에 자동 타입 매핑 기능을 잃게 되었습니다.4. v2에서 권장되는 자동 타입 매핑 방법1. BaseModel + Union + Field(discriminator)v2에서 가장 권장되는 방식은 일반 BaseModel의 필드에 Union과 discriminator를 적용하는 것입니다.from typing import Union, Literalfrom pydantic import BaseModel, Fieldclass MySchema1(BaseModel): type: Literal['schema1'] a: int b: intclass MySchema2(BaseModel): type: Literal['schema2'] a: int c: intclass WrapperModel(BaseModel): data: Union[MySchema1, MySchema2] = Field(discriminator='type')# 사용result = WrapperModel.model_validate({'data': {'type': 'schema1', 'a': 1, 'b': 2}})print(result.data) # MySchema1(type='schema1', a=1, b=2)2. Annotated + TypeAdapter 패턴더욱 직접적인 방법으로는 Annotated와 TypeAdapter를 활용하는 것입니다.from typing import Annotated, Union, Literalfrom pydantic import BaseModel, Field, TypeAdapterclass MySchema1(BaseModel): type: Literal['schema1'] a: int b: intclass MySchema2(BaseModel): type: Literal['schema2'] a: int c: int# Python 3.10 이상에서는 파이프 연산자 사용 가능MyUnionType = Annotated[ MySchema1 | MySchema2, # Union[MySchema1, MySchema2]와 동일 Field(discriminator='type')]# TypeAdapter 사용adapter = TypeAdapter(MyUnionType)result = adapter.validate_python({'type': 'schema1', 'a': 1, 'b': 2})print(result) # MySchema1(type='schema1', a=1, b=2)3. Python 3.10 이상에서의 파이프 연산자Python 3.10 이상에서는 PEP 604에 따라 Union 대신 파이프 연산자(|)를 사용할 수 있습니다.# Python 3.10 이상MyUnionType = Annotated[MySchema1 | MySchema2, Field(discriminator='type')]# Python 3.9 이하MyUnionType = Annotated[Union[MySchema1, MySchema2], Field(discriminator='type')]5. v1/v2 호환 코드 작성 전략조건부 임포트 패턴두 버전을 모두 지원해야 하는 경우, 조건부 임포트를 활용할 수 있습니다.from typing import Annotated, Union, Literalfrom pydantic import BaseModel, Fieldtry: from pydantic import TypeAdapter # v2 환경 def create_parser(union_type): return TypeAdapter(union_type)except ImportError: # v1 환경 from pydantic import parse_obj_as def create_parser(union_type): return lambda data: parse_obj_as(union_type, data)# 공통 타입 정의MyUnionType = Annotated[Union[MySchema1, MySchema2], Field(discriminator='type')]parser = create_parser(MyUnionType)6. 구현 방향성과 모범 사례1. 명확한 용도 구분 단일 값 래핑이 목적이라면 RootModel을 사용하세요 자동 타입 매핑이 목적이라면 Annotated + Union + Field(discriminator) 패턴을 사용하세요2. Discriminator 필드 설계모든 스키마에 공통으로 존재하는 discriminator 필드를 반드시 정의하세요.class Schema1(BaseModel): type: Literal['type1'] # discriminator 필드 # 기타 필드들...class Schema2(BaseModel): type: Literal['type2'] # discriminator 필드 # 기타 필드들...3. 성능과 에러 처리 고려Discriminated Union은 일반 Union보다 빠른 검증 속도와 명확한 에러 메시지를 제공합니다. Pydantic 공식 문서에 따르면, discriminated union의 로직이 Rust로 구현되어 있어 성능상 큰 이점이 있습니다. 따라서 가능한 한 discriminator를 활용하는 것이 좋습니다.4. 중첩된 Discriminator 활용복잡한 타입 구조에서는 중첩된 discriminator를 활용할 수 있습니다.# 먼저 색깔로 구분Cat = Annotated[Union[BlackCat, WhiteCat], Field(discriminator='color')]# 그 다음 동물 종류로 구분Pet = Annotated[Union[Cat, Dog], Field(discriminator='pet_type')]7. 결론 및 권장사항Pydantic의 RootModel은 단일 값 래핑이라는 명확한 목적을 가지고 설계되었습니다. v1에서 가능했던 자동 타입 매핑은 부수적인 활용법이었으며, v2에서는 이를 위한 별도의 패턴이 권장됩니다.최종 권장사항 새로운 프로젝트: Python 3.10 이상이라면 파이프 연산자와 TypeAdapter를 활용하세요 기존 프로젝트 마이그레이션: Annotated + Union + Field(discriminator) 패턴으로 점진적 마이그레이션하세요 v1/v2 호환성: 조건부 임포트나 호환성 라이브러리를 고려하세요위 권장사항들은 저의 개발 과정에서 마주친 문제들을 해결하기 위해 조사한 결과입니다. v1/v2 호환성을 위해 이 문제를 조사하게 되었는데, RootModel의 자동 타입 매핑 기능이 v2에서 제거되면서 기존 코드를 어떻게 마이그레이션해야 할지 고민이 많았거든요.조사 결과 TypeAdapter + Annotated 패턴이 가장 안전하고 공식적인 방법이라는 것이라고 판단했습니다. 여기서 정리한 내용들이 다른 분들에게도 도움이 되길 바랍니다.이러한 접근 방식을 통해 Pydantic의 강력한 타입 시스템을 최대한 활용하면서도, 각 버전의 설계 철학에 맞는 코드를 작성할 수 있으리라 생각합니다.결국 도구의 본래 목적을 이해하고 적절한 패턴을 선택하는 것이, 유지보수 가능하고 안정적인 코드를 만드는 핵심이 아닐까.. 합니다." }, { "title": "NumPy 1.24 이후 np.float 타입 제거 및 대응 전략", "url": "/posts/numpy-float-deprecation/", "categories": "Tech, Python", "tags": "numpy, python", "date": "2025-06-06 00:00:00 +0900", "snippet": "NumPy를 사용하다 보면 가끔 예상치 못한 버전 업데이트 변경점에 당황할 때가 있습니다. NumPy 1.24 버전에서 np.float와 같은 일부 타입들이 완전히 제거된 것이 바로 그런 경우입니다. 많은 기존 코드에 영향을 줄 수 있는 이 변화는 사실 NumPy의 타입 시스템을 더 명확하게 만들고, 오랜 기간 존재했던 혼란을 해결하기 위한 중요한 발...", "content": "NumPy를 사용하다 보면 가끔 예상치 못한 버전 업데이트 변경점에 당황할 때가 있습니다. NumPy 1.24 버전에서 np.float와 같은 일부 타입들이 완전히 제거된 것이 바로 그런 경우입니다. 많은 기존 코드에 영향을 줄 수 있는 이 변화는 사실 NumPy의 타입 시스템을 더 명확하게 만들고, 오랜 기간 존재했던 혼란을 해결하기 위한 중요한 발전입니다.이번 글에서는 이 변화가 왜 필요했는지, 그리고 우리의 코드를 어떻게 수정해야 하는지 구체적인 전략을 알아보겠습니다.np.float는 왜 사라졌을까?결론부터 말하면, np.float가 혼란의 주범이었기 때문입니다.정체성의 혼란np.float의 정체는 사실 Python 내장 float의 또 다른 이름(alias) 이었습니다. 하지만 이름 때문에 많은 개발자들이 NumPy가 제공하는 고유한 숫자 타입으로 오해하곤 했습니다.이로 인해 몇 가지 문제가 발생했습니다- 개발자들이 np.float를 NumPy 고유의 타입으로 오해 Python float와의 관계가 불분명 코드의 가독성과 명확성 저하단계적 제거 과정NumPy 개발팀은 이 문제를 단계적으로 해결했습니다. NumPy 1.20 (2021년 1월): np.float 등의 별칭이 처음 deprecated됨 NumPy 1.20~1.23: DeprecationWarning 발생하지만 여전히 사용 가능 NumPy 1.24 (2023년 6월): 완전히 제거되어 AttributeError 발생실제 에러 메시지는 다음과 같습니다.AttributeError: module 'numpy' has no attribute 'float'. `np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations명확성을 위한 결정이런 혼란을 바로잡기 위해 NumPy 개발팀은 타입 시스템의 일관성을 높이는 방향으로 결정을 내렸습니다.이제는 모호한 np.float 대신, 아래와 같이 의도를 명확하게 드러내는 타입을 사용해야 합니다.# 혼동을 야기하던 과거 방식 (제거됨)np.float(3.14) # Python float? NumPy float?# 명확하고 직관적인 현재 방식 (권장)float(3.14) # Python 내장 타입 사용np.float64(3.14) # NumPy 64비트 부동소수점 타입 사용np.float32(3.14) # NumPy 32비트 부동소수점 타입 사용float vs np.float64np.float가 사라졌으니 이제 Python의 float와 NumPy의 np.float64 사이의 관계를 명확히 이해하는 것이 중요합니다. 이 둘은 사실상 거의 동일한 존재입니다.Python의 float는 C언어의 double을 기반으로 구현되어 있으며, 이는 IEEE 754 표준의 64비트 부동소수점 표현을 사용합니다. NumPy의 np.float64 역시 마찬가지입니다.import numpy as np# 동일한 정밀도와 범위를 가집니다.python_float = 5.9975numpy_float64 = np.float64(5.9975)# 내부 표현도 동일합니다.print(python_float.hex()) # '0x1.7fd70a3d70a3dp+2'print(numpy_float64.hex()) # '0x1.7fd70a3d70a3dp+2'# 값도 당연히 같습니다.print(python_float == numpy_float64) # True그래도 차이는 있습니다메모리 표현 방식은 같지만, Python 타입 시스템의 관점에서 보면 둘은 엄연히 다릅니다.# 타입 확인 결과는 다릅니다.isinstance(2.0, float) # Trueisinstance(2.0, np.float64) # Falseisinstance(np.float64(2.0), float) # False# 사용 가능한 메서드가 다릅니다.np.float64(5.9975).sum() # NumPy 객체이므로 NumPy 메서드 사용 가능(5.9975).sum() # Python float에서는 AttributeError 발생이러한 차이점 때문에 np.float를 단순히 float로 바꿀지, np.float64로 바꿀지는 코드의 맥락에 따라 결정해야 합니다.안전한 마이그레이션 전략그렇다면 기존 코드를 어떻게 수정해야 할까요? 다행히 마이그레이션은 그리 복잡하지 않습니다.1. 코드 수정 방법기본 변환단순한 값 변환에는 Python 내장 float를 사용하는 것이 가장 간단합니다. NumPy 타입이 꼭 필요한 경우에는 np.float64를 사용합니다.# Beforedata = np.float(user_input)# After (간단한 경우)data = float(user_input)# 또는 (NumPy 타입이 필요한 경우)data = np.float64(user_input)배열 타입 지정np.array의 dtype을 지정할 때가 가장 흔한 경우입니다. 이 역시 float나 np.float64로 바꿔주면 됩니다.# Beforearr = np.array([1, 2, 3], dtype=np.float)# Afterarr = np.array([1, 2, 3], dtype=float) # 가장 간단하고 일반적인 방법# 또는arr = np.array([1, 2, 3], dtype=np.float64) # 의도를 명시적으로 드러내는 방법2. 구버전 호환성 유지만약 작성하는 코드가 구버전 NumPy와도 호환되어야 한다면, 다음과 같이 예외 처리를 활용할 수 있습니다.import numpy as np# 이전 버전과의 호환성을 위한 처리try: # NumPy 1.20+ 에서는 numpy.float가 DeprecationWarning을 발생시키며 여전히 존재 # 1.24에서 완전히 제거됨 from numpy import float as np_floatexcept ImportError: # np.float가 없는 최신 버전에서는 Python float를 사용 np_float = float# 이제 np_float를 안전하게 사용 가능arr = np.array([1, 2, 3], dtype=np_float)하지만 라이브러리를 개발하는 경우가 아니라면, 코드를 최신 버전에 맞게 수정하는 것을 더 권장합니다.3. 대규모 코드베이스 한번에 바꾸기프로젝트 전체에 np.float가 퍼져있다면, 자동화된 스크립트로 한 번에 수정하는 것이 효율적입니다.정규표현식 활용 (Linux/macOS)sed와 같은 커맨드라인 도구를 사용하면 빠르게 변경할 수 있습니다.# np.float(를 float(로 변경sed -i 's/np\\.float(/float(/g' **/*.py# dtype=np.float를 dtype=float로 변경sed -i 's/dtype=np\\.float\\b/dtype=float/g' **/*.py 주의- sed -i 명령어는 OS나 버전에 따라 동작이 다를 수 있으니, 실행 전 반드시 코드를 백업하세요.Python 스크립트로 안전하게 바꾸기플랫폼에 상관없이 더 안전하게 코드를 변경하고 싶다면 Python 스크립트를 작성하는 것이 좋습니다.import reimport osfrom pathlib import Pathdef migrate_numpy_float(file_path): # 파일을 UTF-8로 안전하게 읽기 try: content = file_path.read_text(encoding='utf-8') except Exception as e: print(f\"파일 읽기 오류 {file_path}: {e}\") return # np.float( -&gt; float( content_new = re.sub(r'np\\.float\\(', 'float(', content) # dtype=np.float -&gt; dtype=float content_new = re.sub(r'dtype=np\\.float\\b', 'dtype=float', content_new) # np.float_ -&gt; np.float64 (NumPy 2.0 대비) content_new = re.sub(r'np\\.float_', 'np.float64', content_new) if content != content_new: print(f\"마이그레이션 적용- {file_path}\") file_path.write_text(content_new, encoding='utf-8')# 현재 디렉토리 및 하위 디렉토리의 모든 .py 파일을 대상으로 실행for py_file in Path('.').glob('**/*.py'): migrate_numpy_float(py_file)이 스크립트는 np.float_처럼 곧 사라질 다른 타입들까지 함께 처리해줄 수 있어 더욱 유용합니다.앞으로의 코딩 습관이번 변화를 계기로 삼아 더 좋은 코딩 습관을 기를 수 있습니다.1. 명시적인 타입 사용하기가장 중요한 것은 타입을 명시적으로 사용하는 습관입니다. 모호한 별칭 대신 정확한 타입을 사용하면 코드의 의도가 분명해집니다.# 좋은 예: 의도가 명확함def process_data(values): return np.array(values, dtype=np.float64)# 나쁜 예: 모호함 (제거된 방식)def process_data(values): return np.array(values, dtype=np.float)2. 타입 힌트 적극 활용하기Python의 타입 힌트를 함께 사용하면 코드의 안정성을 더욱 높일 수 있습니다.from typing import Unionimport numpy as npfrom numpy.typing import NDArraydef calculate_mean(data: Union[list, NDArray]) -&gt; np.float64: # np.mean은 기본적으로 float64를 반환하지만, dtype을 명시하여 의도를 확실히 할 수 있음 return np.mean(data, dtype=np.float64)3. 성능이 중요하다면 정밀도 선택하기모든 부동소수점이 np.float64일 필요는 없습니다. 데이터의 특성에 따라 적절한 정밀도를 선택하면 메모리를 효율적으로 사용할 수 있습니다.# 일반적인 정밀도로 충분하고 메모리 사용량이 중요할 때small_precision_array = np.zeros(100, dtype=np.float32) # 400 bytes# 높은 정밀도가 반드시 필요할 때high_precision_array = np.zeros(100, dtype=np.float64) # 800 bytesNumPy 2.0에서는 np.float_도 제거됩니다NumPy 2.0에서는 밑줄이 하나 붙은 np.float_도 제거될 예정입니다. 이것 역시 np.float64의 별칭이었기 때문입니다.NumPy 2.0 마이그레이션 가이드에 따르면, 메인 네임스페이스에서 약 100개의 멤버가 deprecated, 제거 또는 이동되었습니다.마이그레이션을 진행할 때 이 부분도 함께 np.float64로 변경해주는 것이 좋습니다.결론NumPy 1.24의 np.float 제거는 처음에는 당황스러울 수 있는 ‘breaking change’지만, 그 본질을 들여다보면 NumPy가 더 나은 방향으로 발전하고 있다는 신호입니다. 코드의 모호성을 줄이고 타입 시스템의 일관성을 높이려는 노력의 일환인 셈이죠.이제 우리는 np.float 대신 Python의 내장 float나 명시적인 np.float64를 사용하면 됩니다. 이 둘은 메모리 표현이나 정밀도 면에서 거의 동일하기에 마이그레이션 부담도 적습니다. 이번 기회에 코드 베이스를 점검하고 더 명확한 코드로 개선해 보는 것은 어떨까요?앞으로는 타입을 명시적으로 지정하는 습관을 통해, NumPy의 발전 방향에 발맞춰 더욱 견고하고 가독성 좋은 코드를 작성해 나가야겠습니다." }, { "title": "Airflow DAG 파싱 주기 설정 시 주의할 점과 최적화 전략", "url": "/posts/airflow-dag-parsing-optimization/", "categories": "Tech, Airflow", "tags": "Airflow", "date": "2025-05-24 00:00:00 +0900", "snippet": "서론Airflow를 운영하다 보면 DAG 변경사항이 언제 반영되는지 답답할 때가 많습니다.파일을 수정했는데 UI에서 바로 확인이 안 되고, “도대체 언제 반영되는 거야?” 싶어 F5를 연타한 경험, 다들 있으실 거예요.저도 처음에는 단순히 “파싱 주기만 짧게 하면 되겠지”라고 생각했는데, 무작정 적용했다가는 또 다른 문제가 터질 수 있습니다.오늘은 실...", "content": "서론Airflow를 운영하다 보면 DAG 변경사항이 언제 반영되는지 답답할 때가 많습니다.파일을 수정했는데 UI에서 바로 확인이 안 되고, “도대체 언제 반영되는 거야?” 싶어 F5를 연타한 경험, 다들 있으실 거예요.저도 처음에는 단순히 “파싱 주기만 짧게 하면 되겠지”라고 생각했는데, 무작정 적용했다가는 또 다른 문제가 터질 수 있습니다.오늘은 실무에서 겪었던 파싱 주기 설정 시 주의할 점들과 나름대로 찾아낸 최적화 팁들을 공유해보려고 합니다.목차 1. Airflow DAG 파싱 과정 이해하기 2. 주요 옵션별 역할과 동작 방식 3. 왜 파싱만으로는 의미가 없는가? 4. 설정 시 주의점 &amp; 권장 전략 5. 결론 및 개인적인 소감1. Airflow DAG 파싱 과정 이해하기1.1 전체 파싱 흐름도DAG 파일 변경/추가 ↓① 디렉토리 스캔 (refresh_interval - Airflow 3.0+) ↓ ② DAG 파일 파싱 (min_file_process_interval) ↓③ 직렬화 &amp; DB 저장(CLI 반영) (min_serialized_dag_update_interval) ↓④ Webserver 읽기 (min_serialized_dag_fetch_interval) ↓⑤ UI/REST API에 반영1.2 각 단계별 상세 설명① 디렉토리 스캔 DAG 프로세서가 DAGs 폴더를 주기적으로 스캔하여 새로운 파일이나 삭제된 파일을 감지 파일의 last modified date, 크기 등 메타데이터 확인② DAG 파일 파싱 .py로 작성된 DAG 코드를 실행하여 DAG 객체 파싱 외부 임포트, 환경 변수, 동적 구성 요소 모두 처리 여기서 중요한 건, 파일이 변경되지 않아도 외부 의존성 때문에 주기적 파싱이 필요하다는 점입니다③ 직렬화 &amp; DB 저장 파싱된 DAG 객체를 JSON 형태로 직렬화 메타데이터 DB의 serialized_dag 테이블에 저장 이 단계가 완료되어야 비로소 다른 컴포넌트들이 변경사항을 인식합니다④ Webserver 읽기 Webserver가 DB에서 직렬화된 DAG 정보를 읽어옴 메모리에 DagBag 구성하여 UI/REST API에서 사용1.3 왜 풀파싱(전체 파싱)이 필요한가?Airflow에서 단순히 파일 변경만 감지하지 않고 주기적으로 모든 DAG을 파싱하는 이유는 다음과 같습니다 동적 DAG 생성 - 환경 변수, 데이터베이스 조회, API 호출로 DAG 구조가 동적으로 결정 외부 의존성 - 임포트된 모듈, 설정 파일이 바뀌면 DAG 파일은 그대로여도 파싱 결과 변경 신뢰성 우선 - 변경 감지 실패로 인한 DAG 동기화 문제를 방지2. 주요 옵션별 역할과 동작 방식2.1 [dag_processor] refresh_interval (Airflow 3.0+) 기능 - DAG 번들에서 새 파일을 찾거나 갱신하는 주기 (초 단위) 기본값 - 300 (5분) 역할 - 새 파일 추가/삭제 감지 Airflow 2.x에서는 - [scheduler] dag_dir_list_interval 사용 (deprecated) ⚠️ min_file_process_interval보다 짧게 설정하면 불필요한 스캔이 발생합니다2.2 [dag_processor] min_file_process_interval 기능 - 각 DAG 파일을 파싱하는 최소 간격 (초 단위) 기본값 - 30 역할 파일 변경 여부와 무관하게 주기적으로 전체 DAG 파싱 외부 의존성(임포트 모듈, 환경 변수) 변화 감지 Tip 파싱이 모든 변화의 시작점이므로, 이 옵션이 DAG 반영 속도에 가장 직접적인 영향을 줍니다 2.3 [core] min_serialized_dag_update_interval 기능 - 직렬화된 DAG을 DB에 저장하는 최소 간격 (초 단위) 기본값 - 30 역할 - 파싱 결과를 메타데이터 DB에 저장 ⚠️ 중요 이 단계가 완료되어야 CLI가 변경사항을 인식할 수 있습니다 2.4 [core] min_serialized_dag_fetch_interval 기능 - Webserver가 DB에서 직렬화된 DAG을 읽어오는 최소 간격 (초 단위) 기본값 - 10 역할 - UI/REST API 반영 속도 제어3. 왜 파싱만으로는 의미가 없는가?3.1 파싱 과정과 저장 과정이 분리되어 있음문제 시나리오[dag_processor]min_file_process_interval = 10 # 10초마다 파싱[core]min_serialized_dag_update_interval = 300 # 5분마다 DB 저장결과 DAG 프로세서는 10초마다 파싱하여 최신 DAG 구조 인식 하지만 DB 저장은 5분마다만 수행 Webserver/UI는 5분 후에야 변경사항 확인 가능3.2 Airflow 2.x/3.x의 직렬화 필수 구조Airflow 2.0부터 DAG 직렬화가 필수가 되었고 비활성화할 수 없습니다. DAG 프로세서/스케줄러 - DAG 파싱 + 직렬화 + DB 저장 담당 Webserver - DAG 파일을 직접 파싱하는 것이 금지되고, DB의 직렬화된 정보만 사용 CLI - 마찬가지로 DB의 직렬화된 정보를 참조결론 - 파싱 결과가 DB에 저장되지 않으면, 아무리 자주 파싱해도 시스템 전체에 변경사항이 반영되지 않습니다.4. 설정 시 주의점 &amp; 권장 전략4.1 옵션 간 밸런스가 핵심최적화 예제refresh_interval ≥ min_file_process_interval ≈ min_serialized_dag_update_interval권장 설정 예시Airflow 3.0+:[dag_processor]refresh_interval = 300min_file_process_interval = 30[core]min_serialized_dag_update_interval = 30min_serialized_dag_fetch_interval = 10Airflow 2.x:[scheduler]dag_dir_list_interval = 300 # deprecated but works in all versions[dag_processor]min_file_process_interval = 30[core]min_serialized_dag_update_interval = 30min_serialized_dag_fetch_interval = 104.2 환경별 튜닝 가이드소규모 환경 (DAG &lt; 100개) 빠른 반영 우선: min_file_process_interval = 10 동기화: min_serialized_dag_update_interval = 10대규모 환경 (DAG 1000+개) 안정성 우선: min_file_process_interval = 60 DB 부하 고려: min_serialized_dag_update_interval = 120 압축 활성화: compress_serialized_dags = True (단, DAG 의존성 뷰가 비활성화됨)4.3 모니터링 추천 지표성능 메트릭 dag_processing.total_parse_time: 전체 파싱 소요 시간 dag_processing.last_duration: 마지막 파싱 소요 시간 scheduler.heartbeat: 스케줄러가 살아있는지 확인임계점 판단 total_parse_time &gt; min_file_process_interval이면 파싱 주기를 늘려야 합니다 Webserver 메모리 사용량이 급증하면 min_serialized_dag_fetch_interval을 늘려야 합니다5. 결론 및 개인적인 소감5.1 우선순위 설정 원칙 min_file_process_interval을 기준으로 삼으세요 파싱이 모든 변화의 시작점이므로, 이 값을 가장 신중하게 관리해야 합니다 DB 저장 주기는 파싱 주기와 동기화 파싱만 하고 저장하지 않으면 의미가 없습니다 min_serialized_dag_update_interval = min_file_process_interval 권장 Webserver 반영 속도는 기존 DAG 운영 관점에서는 부차적 UI 지연 허용 범위 내에서 min_serialized_dag_fetch_interval 조정 단, RestAPI를 통해 Airflow를 제어하고 있다면 이 값 역시 중요할 수 있음 5.2 개인적인 경험담이런 설정들을 일부 만져본 경험으로는, 모든 주기를 바로 짧게 만들기보다는 점진적으로 최적화하는 것을 추천합니다.특히 파싱 주기를 너무 짧게 설정한다면 CPU 사용량이 급증할 수 있으니 전체 시스템이 불안정해질 수 있으니, 항상 모니터링을 하면서 조정하는 것을 권장드립니다.가장 중요한 건, “파싱은 시작이지만, 저장 없이는 끝나지 않는다”는 점입니다.빠른 파싱보다는 일관된 파싱-저장 주기를 유지하는 것이 안정적인 Airflow 운영의 핵심이라고 생각합니다.참고 자료Apache Airflow DAG Serialization 공식 문서Apache Airflow Release Notes" }, { "title": "git 브랜치명에 언더바를 잘 안 쓰는 이유", "url": "/posts/git-branch-naming-without-underscore/", "categories": "Tech, git", "tags": "git, naming-convention", "date": "2025-05-18 00:00:00 +0900", "snippet": "Git을 사용하다 보면 브랜치명을 어떻게 지어야 할지 고민하는 경우가 많습니다. 특히 브랜치명에서 단어를 구분할 때 하이픈-과 언더바_ 중 무엇을 써야 할지 고민이 되는데, 실제로 오픈소스 프로젝트나 실무에서는 하이픈-을 압도적으로 많이 사용하고, 언더바_는 거의 사용하지 않습니다.이번 글에서는 왜 git 브랜치명에 언더바를 잘 안 쓰는지, 그리고 이...", "content": "Git을 사용하다 보면 브랜치명을 어떻게 지어야 할지 고민하는 경우가 많습니다. 특히 브랜치명에서 단어를 구분할 때 하이픈-과 언더바_ 중 무엇을 써야 할지 고민이 되는데, 실제로 오픈소스 프로젝트나 실무에서는 하이픈-을 압도적으로 많이 사용하고, 언더바_는 거의 사용하지 않습니다.이번 글에서는 왜 git 브랜치명에 언더바를 잘 안 쓰는지, 그리고 이런 관례가 어떻게 자리잡게 되었는지 살펴보고, 마지막으로 오픈소스 프로젝트에서 널리 쓰이는 브랜치 네이밍 컨벤션까지 정리해보겠습니다.왜 언더바(_)를 잘 안 쓸까?1. 관례(Convention)의 힘사실 기술적으로 git 브랜치명에는 하이픈-과 언더바_ 모두 사용할 수 있습니다. 하지만 하이픈-을 쓰는 것이 업계 표준처럼 자리잡은 가장 큰 이유는 바로 관례(Convention) 때문입니다.수많은 오픈소스 프로젝트, 글로벌 개발자 커뮤니티, 그리고 여러 공식 가이드에서 하이픈-을 단어 구분자로 권장하고 있기 때문입니다.2. 가독성과 일관성하이픈-은 영어권에서 단어를 연결하는 표준적인 방식이고, URL, 파일명 등 다양한 환경에서 문제없이 사용할 수 있고, 사용되고 있습니다.브랜치명을 여러 사람이 함께 볼 때 하이픈-이 단어 구분이 더 명확하고, 협업 시 혼동이 적어 일관성을 유지하기 쉽다, 라고들 이야기합니다.3. 호환성과 실무적 이유 하이픈-은 GitHub, GitLab 등 주요 플랫폼에서 URL 경로로 아무런 문제 없이 동작합니다. 언더바_는 변수명 등 코드 내부에서 주로 쓰이고, URL에서 가독성이 떨어지거나 가끔 혼동을 줄 수 있습니다. 일부 명령줄 환경이나 스크립트에서 하이픈과 언더바를 혼동할 수 있어, 오타나 실수를 줄이기 위해서도 하이픈이 선호되기도 합니다.4. 코드와 브랜치명의 환경 차이프로그래밍 언어와 Git 브랜치명 사이에 구분자 선택이 다른 근본적인 이유가 있습니다.코드(프로그래밍 언어)에서는 하이픈-이 대부분의 언어에서 뺄셈 연산자로 인식되기 때문에 변수명, 함수명 등의 식별자에 사용할 수 없습니다. 그래서 언더바_가 자연스럽게 단어 구분자로 자리잡았습니다. Python의 snake_case, C/C++의 상수명, Java의 일부 네이밍 등에서 언더바가 널리 사용됩니다.반면, Git 브랜치명, 파일명, URL 등에서는 이런 환경은 “연산이 불가능한 환경”이므로 하이픈-을 자유롭게 사용할 수 있습니다. 하이픈-이 단어 구분이 명확하고 가독성이 좋아 기본 구분자로 널리 쓰이게 되었습니다.즉, 환경에 따라 자연스럽게 네이밍 컨벤션이 달라진 것입니다 연산이 필요한 환경(코드): 언더바_가 기본 구분자 연산이 불가능한 환경(브랜치명/URL): 하이픈-이 기본 구분자어쩌다 관례가 되었을까?이러한 네이밍 규칙은 공식적인 표준이 정해진 것이 아니라,수많은 오픈소스 프로젝트와 글로벌 개발자들이 하이픈-을 꾸준히 사용하면서 자연스럽게 업계 관례가 된 것입니다.특히 대형 오픈소스 프로젝트(예: React, TensorFlow, VS Code 등)에서 하이픈-을 적극적으로 사용하고,GitHub, Atlassian, Microsoft 등 주요 개발 플랫폼의 가이드에서도 하이픈- 사용을 권장하면서초보 개발자부터 시니어까지 모두가 하이픈-을 쓰는 문화가 굳어져 발생한 현상입니다.오픈소스 프로젝트의 브랜치 네이밍 컨벤션 정리1. 기본 구조 카테고리/설명(혹은 이슈번호-설명) 형태가 기본입니다. 카테고리: feature, bugfix, hotfix, refactor, docs 등 구분자: 슬래시(/)로 카테고리와 설명을 나누고, 하이픈-으로 단어를 구분합니다.예시feature/add-user-profilebugfix/fix-login-errorhotfix/urgent-patchrefactor/optimize-database-queriesdocs/update-readmefeature/123-add-login2. 브랜치명 작성 팁 짧고 명확하게: 브랜치명은 간결하면서도 목적이 잘 드러나야 합니다. 소문자 사용: 대문자보다 소문자를 권장합니다. 특수문자 지양: 공백, 특수문자(#, %, &amp; 등)는 피하고, 하이픈·슬래시만 사용. 일관성 유지: 프로젝트 내에서 컨벤션을 문서화하고, 팀원 전체가 일관되게 사용해야 합니다.3. 참고 링크 Microsoft Learn: Create a new Git branch Google Cloud Looker: Using version control and deploying How to Contribute to Apache Hive마치며정리하자면, git 브랜치명에서 언더바_를 잘 안 쓰는 이유는 기술적 제약이 아니라 관례와 협업의 효율성 때문입니다.하이픈-이 가독성, 호환성, 일관성 면에서 유리하고,대부분의 오픈소스 프로젝트와 글로벌 개발 커뮤니티에서 이를 표준처럼 지금까지 사용해 오고 있습니다.우리들 또한 해당 도구들을 이용한다면, 브랜치 네이밍 컨벤션을 정할 때이런 히스토리와 문화를 이해하는 것도 협업과 유지보수를 용이하게 하는데 도움이 될 수 있습니다." }, { "title": "파이썬 개발자가 가져야 할 자세", "url": "/posts/python-keeps-you-on-your-toes/", "categories": "Tech, Python", "tags": "Python, 잡설", "date": "2025-05-17 00:00:00 +0900", "snippet": "서론파이썬은 지속적으로 발전하는 프로그래밍 언어로, 버전 업그레이드마다 새로운 기능과 변경사항이 도입됩니다.이러한 변화는 기존 코드의 동작에 대한 영향을 매우 크게 강요하는 경우도 꽤나 흔합니다.특히 이전 글에서 설명한 Python 3.11에서 str과 Enum의 동작 변경은 많은 개발자들에게 주의가 필요한 단적인 예시입니다.목차 1. 파이썬의 지속...", "content": "서론파이썬은 지속적으로 발전하는 프로그래밍 언어로, 버전 업그레이드마다 새로운 기능과 변경사항이 도입됩니다.이러한 변화는 기존 코드의 동작에 대한 영향을 매우 크게 강요하는 경우도 꽤나 흔합니다.특히 이전 글에서 설명한 Python 3.11에서 str과 Enum의 동작 변경은 많은 개발자들에게 주의가 필요한 단적인 예시입니다.목차 1. 파이썬의 지속적인 변화와 대응 2. 개발자의 준비 자세 3. 파이썬의 설계 철학 이해하기 4. 호환성 관리 전략 5. 그래서 파이썬 말고 다른 대안이 있는가?1. 파이썬의 지속적인 변화와 대응파이썬은 하위 호환성을 중요시하면서도, 때로는 언어의 명확성과 개선을 위해 기존 동작을 변경하기도 합니다. 이러한 변화는 개발자들에게 지속적인 학습과 적응을 강요합니다.2. 개발자의 준비 자세파이썬 개발자는 항상 최신 버전의 변경사항을 주시하고, 자신의 코드가 새로운 버전에서도 자신의 의도대로 동작하는지 확인해야 합니다…# Python 3.11 이전f\"{MyEnum.VALUE}\" # \"실제 값\"# Python 3.11 이후f\"{MyEnum.VALUE}\" # \"MyEnum.VALUE\"3. 파이썬의 설계 철학 이해하기파이썬은 “명시적인 것이 암시적인 것보다 낫다”는 철학을 가지고 있습니다. 이는 코드의 가독성과 유지보수성을 높이기 위한 중요한 원칙입니다.4. 호환성 관리 전략try: # Python 3.11 이상 from enum import StrEnumexcept ImportError: # 이전 버전 호환성 from enum import Enum class StrEnum(str, Enum): pass5. 그래서 파이썬 말고 다른 대안이 있는가?결국 우리는, 파이썬이 언제라도 변할 수 있다는 사실을 염두에 두고 계속 사용할 수 밖에 없습니다. 저를 포함한 여러분 모두의 다음 버전 마이그레이션이 부드럽게 진행되길 바랍니다. 아마도 그러지 않겠지만요..파이썬은 처음 도입하긴 쉽지만, 지속적인 서비스의 유지를 위해서는 정말 많은 신경을 쏟아야 하는 것 같습니다. 정말 전혀 예상치 못한 곳에서, 예상치 못한 사유로 문제가 되는 일이 비일비재합니다.결국, 파이썬 개발 환경에서는 모든 가능성을 철저히 대비하거나 혹은, 예기치 않은 변화에 대한 유연함을 높이는 양자택일의 상황을 계속 마주하게 되고 끊임없는 선택이 요구됩니다.무엇이 정답인지는 아직 잘 모르겠습니다. 어쩌면 정답은 없을지도요.." }, { "title": "Python 3.10에서 3.11로 마이그레이션 시 str, Enum 문자열 출력 동작 변경과 대응 전략", "url": "/posts/py310-to-py311-str-enum-changes/", "categories": "Tech, Python", "tags": "Python, Python3.11, Enum", "date": "2025-05-16 00:00:00 +0900", "snippet": "서론Python 3.11에서는 str, Enum 다중상속 클래스의 문자열 출력 동작이 크게 변경되었습니다.이 변화는 열거형 클래스의 일관성을 높이기 위한 의도적인 수정이지만, 이미 작성된 코드 호환성에 직접적인 영향을 주므로 주의가 필요합니다.목차 1. Python 3.10과 3.11의 차이점 2. 영향받는 코드 3. 해결책: StrEnum 사용...", "content": "서론Python 3.11에서는 str, Enum 다중상속 클래스의 문자열 출력 동작이 크게 변경되었습니다.이 변화는 열거형 클래스의 일관성을 높이기 위한 의도적인 수정이지만, 이미 작성된 코드 호환성에 직접적인 영향을 주므로 주의가 필요합니다.목차 1. Python 3.10과 3.11의 차이점 2. 영향받는 코드 3. 해결책: StrEnum 사용하기 4. 하위 호환성 유지 방법 5. 테스트 권장사항 6. 결론1. Python 3.10과 3.11의 차이점# Python 3.10from enum import Enumclass Color(str, Enum): RED = 'red' GREEN = 'green' BLUE = 'blue'print(f\"{Color.RED}\") # 'red'print(str(Color.RED)) # 'Color.RED'print(\"{}\".format(Color.RED)) # 'red'print(\"%s\" % Color.RED) # 'Color.RED'# Python 3.11from enum import Enumclass Color(str, Enum): RED = 'red' GREEN = 'green' BLUE = 'blue'print(f\"{Color.RED}\") # 'Color.RED' - 변경됨!print(str(Color.RED)) # 'Color.RED'print(\"{}\".format(Color.RED)) # 'Color.RED' - 변경됨!print(\"%s\" % Color.RED) # 'Color.RED'이 변경은 Python의 Enum 클래스 동작을 더 일관되게 만들기 위한 의도적인 수정입니다.Python 3.10까지는 str, Enum 다중상속 클래스가 f-string이나 str.format()에서 사용될 때 __format__ 메서드가 값을 반환했지만,str() 함수나 %-형식 문자열에서는 클래스와 멤버 이름을 반환했습니다. 3.11부터는 이 불일치가 해소되어 모든 문자열 변환 상황에서 클래스명과 멤버명을 함께 출력하도록 바뀌었습니다12.2. 영향받는 코드이 변경은 다음과 같은 상황에서 문제를 일으킬 수 있습니다: f-string에서 Enum 멤버를 직접 사용하는 코드 str.format()을 사용한 포맷팅 Enum 값이 문자열로 자동 변환되는 API 호출# 3.10에선 작동하나 3.11에선 실패하는 코드def make_path(color: Color) -&gt; str: return f\"/colors/{color}\" # 3.10: \"/colors/red\", 3.11: \"/colors/Color.RED\"3. 해결책: StrEnum 사용하기Python 3.11에서는 이 문제를 해결하기 위해 StrEnum 클래스가 새로 추가되었습니다34.# Python 3.11from enum import StrEnumclass Color(StrEnum): RED = 'red' GREEN = 'green' BLUE = 'blue'print(f\"{Color.RED}\") # 'red'print(str(Color.RED)) # 'red'StrEnum은 문자열 포맷팅 시 값을 반환하도록 설계된 새로운 클래스입니다.기존 Python 3.10의 str, Enum 다중상속이 f-string에서만 값을 반환했던 것과 달리, StrEnum은 모든 문자열 변환 상황에서 일관되게 값을 반환합니다12.4. 하위 호환성 유지 방법Python 3.10 이하 버전과 3.11 이상을 모두 지원하려면:try: from enum import StrEnumexcept ImportError: from enum import Enum class StrEnum(str, Enum): \"\"\"Python 3.10 이하에서 StrEnum 에뮬레이션\"\"\" def __str__(self): return self.value def __format__(self, format_spec): return format(self.value, format_spec)class Color(StrEnum): RED = 'red' GREEN = 'green' BLUE = 'blue'위 코드는 Python 3.11에서는 내장된 StrEnum을 사용하고, 이전 버전에서는 str, Enum 다중상속에 __str__과 __format__ 메서드를 직접 구현하여 동일한 동작을 구현합니다5.5. 테스트 권장사항여러 Python 버전에서의 동작을 확인하기 위해 다음 테스트를 권장합니다:def test_enum_string_format(): assert f\"{Color.RED}\" == \"red\" assert str(Color.RED) == \"red\" assert \"{}\".format(Color.RED) == \"red\" assert \"{:&gt;5}\".format(Color.RED) == \" red\" # 포맷 지정자 테스트 assert f\"{Color.RED:&gt;5}\" == \" red\" # f-string 포맷 지정자 테스트6. 결론이 변경사항은 Python 3.11로 업그레이드하는 많은 프로젝트에 영향을 줄 수 있으므로, 미리 코드를 점검하고 대응하는 것이 중요합니다.특히 f-string이나 문자열 포맷팅을 사용하는 코드는 반드시 테스트를 통해 동작을 확인해야 합니다. https://tsak.dev/posts/python-enum/ &#8617; &#8617;2 https://docs.python.org/3/whatsnew/3.11.html#enum &#8617; &#8617;2 https://docs.python.org/3/howto/enum.html &#8617; https://blog.pecar.me/python-enum &#8617; https://tomwojcik.com/posts/2023-01-02/python-311-str-enum-breaking-change &#8617; " }, { "title": "FFT 압축 결과는 왜 Latent Vector가 아닐까?", "url": "/posts/fft-and-latent-vector/", "categories": "Tech, Machine Learning", "tags": "FFT, Data Compression, Deep Learning", "date": "2025-05-07 00:00:00 +0900", "snippet": "서론FFT(Fast Fourier Transform) 기반 압축과 Latent Vector는 모두 데이터를 효율적으로 표현한다는 공통점이 있지만, 그 메커니즘과 목적에서 근본적인 차이가 있습니다.마치, FFT 압축을 수행한 이미지를 쭉 나열하면 Latent Vector로써 기능할 수 있을 것 같은 착각을 불러옵니다.이 글에서는 두 방법의 작동 원리를 ...", "content": "서론FFT(Fast Fourier Transform) 기반 압축과 Latent Vector는 모두 데이터를 효율적으로 표현한다는 공통점이 있지만, 그 메커니즘과 목적에서 근본적인 차이가 있습니다.마치, FFT 압축을 수행한 이미지를 쭉 나열하면 Latent Vector로써 기능할 수 있을 것 같은 착각을 불러옵니다.이 글에서는 두 방법의 작동 원리를 비교하고, 왜 FFT 압축이 Latent Vector가 아닌지 설명합니다.목차 1. FFT 압축이란? 2. Latent Vector란? 3. FFT 압축과 Latent Vector의 차이 4. 혼동의 원인 5. 실제 예시 6. 결론1. FFT 압축이란?FFT는 데이터를 주파수 영역으로 변환해 고주파 성분을 제거하는 손실 압축 기법입니다.작동 원리 주파수 변환: 이미지/신호를 FFT로 주파수 영역으로 변환 고주파 제거: 인간이 인지하기 어려운 고주파 성분을 임계값(threshold)으로 걸러냄 역변환: 남은 저주파 성분으로 역FFT를 수행해 압축된 데이터 복원한계점 의미 무시: 픽셀/신호의 물리적 특성만 고려하며, 객체 식별이나 맥락 이해와 무관 비학습적: 데이터 분포를 학습하지 않고, 고정된 수학적 규칙(에너지 크기)으로 압축 복원 품질: 고주파 손실로 인해 디테일이 흐릿해짐2. Latent Vector란?Latent Vector는 신경망이 데이터의 본질적 특징을 학습해 추출한 저차원 벡터입니다.작동 원리 학습 단계: 오토인코더, GAN 등이 데이터 분포를 학습 특징 추출: 입력 데이터를 본질적 속성(예: 얼굴 표정, 객체 종류)으로 압축 의미 공간: 유사한 데이터는 벡터 공간에서 가까이 위치장점 맥락 이해: “고양이 vs. 강아지”처럼 의미적 유사성을 포착 유연한 생성: 벡터 연산으로 새로운 데이터 생성 가능 (예: GAN)3. FFT 압축과 Latent Vector의 차이목적의 차이 FFT 압축: 데이터 크기 감소 (물리적 효율성) Latent Vector: 데이터의 의미적 특징 추출 (고수준 표현)압축 기준 FFT 압축: 주파수 영역의 에너지 크기 Latent Vector: 신경망이 학습한 의미적 중요도복원 메커니즘 FFT 압축: 역FFT (수학적 복원) Latent Vector: 디코더 네트워크 (학습 기반 복원)4. 혼동의 원인 공통점: 데이터 크기 감소 오해 포인트: 압축한 벡터 == Latent Vector라고 생각하기 쉽지만 Latent Vector는 의미 공간(latent space)으로의 매핑이 필수 FFT 압축을 수행한 데이터는, 원본 데이터와 개념적으로 동일 5. 실제 예시FFT 압축 (JPEG) 원본: 1024x768 RGB 이미지 (2.4MB) FFT 압축: 고주파 제거 → 300KB JPEG 파일 결과: 디테일 손실 있지만, “산”이라는 객체를 이미지로써 식별 가능. 여전히 이미지 데이터Latent Vector (오토인코더 등) 원본: 동일 이미지 (2.4MB) Latent Vector: 128차원 벡터 (2KB) 결과: 디테일 보존 + “산, 나무, 하늘” 등 의미(제공한 레이블 등)적 특징을 갖는 벡터로 매핑6. 결론FFT 압축은 데이터의 물리적 신호를 단순히 줄이는 도구일 뿐, Latent Vector처럼 의미를 이해하거나 생성하는 능력이 없습니다. FFT 압축 인간의 시각적 인지에 맞춘 손실 압축 threshold에 따라 압축자가 용인 가능한 선에서 원본 데이터의 품질 저하를 수반 Latent Vector 의미적 이해를 기반으로 한 지능적 압축 원본 데이터를 어떠한 의미 공간 속 벡터로 매핑 따라서 두 방법은 목적과 메커니즘에서 근본적으로 다릅니다" }, { "title": "Embedding과 Latent Vector, 헷갈리는 개념 정리", "url": "/posts/embedding-and-latent-vector/", "categories": "Tech, Machine Learning", "tags": "embedding, Deep Learning", "date": "2025-05-06 00:00:00 +0900", "snippet": "서론머신러닝과 딥러닝에서 embedding, embedding vector, latent vector는 종종 혼용되거나 모호하게 사용되는 개념입니다.이 세 용어는 서로 연관되어 있지만 엄밀히는 다른 의미를 가지며, 혼동할 경우 모델 해석이나 구현에 실수가 발생할 수 있습니다.이 글에서는 각 개념의 정의와 차이를 명확히 정리해보겠습니다.목차 1. Em...", "content": "서론머신러닝과 딥러닝에서 embedding, embedding vector, latent vector는 종종 혼용되거나 모호하게 사용되는 개념입니다.이 세 용어는 서로 연관되어 있지만 엄밀히는 다른 의미를 가지며, 혼동할 경우 모델 해석이나 구현에 실수가 발생할 수 있습니다.이 글에서는 각 개념의 정의와 차이를 명확히 정리해보겠습니다.목차 1. Embedding이란? 2. Embedding Vector란? 3. Latent Vector란? 4. 개념 간 관계 5. 실제 적용 사례 6. 결론1. Embedding이란?Embedding은 데이터를 의미적 관계를 보존하는 연속 벡터 공간으로 변환하는 과정이라고 할 수 있으며, 이 과정에서 차원은 목적과 모델에 따라 증가하거나 감소할 수 있습니다.핵심 특징 목적: 데이터를 벡터 공간에 매핑하여 의미적 관계를 수치적으로 표현 특성: 입력 데이터보다 차원이 축소될 수도, 확장될 수도 있음 적용: 자연어 처리, 추천 시스템, 컴퓨터 비전 등 다양한 분야주요 임베딩 방법 Word2Vec, GloVe: 단어 임베딩 BERT, RoBERTa: 문맥 기반 임베딩 Node2Vec: 그래프 임베딩 nn.Embedding: 딥러닝 모델의 임베딩 레이어2. Embedding Vector란?Embedding Vector는 임베딩 과정을 통해 생성된 실제 벡터를 의미합니다.핵심 특징 형태: 고정된 차원의 실수 벡터 (예: 300차원, 768차원) 특성: 유사한 의미의 데이터는 벡터 공간에서 가까이 위치 유연성: 원본 데이터의 차원보다 크거나 작을 수 있음예시# PyTorch에서 임베딩 벡터 생성embedding_layer = nn.Embedding(vocab_size=10000, embedding_dim=64)input_ids = torch.tensor([1, 2, 3])embedding_vectors = embedding_layer(input_ids) # 결과: (3, 64) 크기의 텐서3. Latent Vector란?Latent Vector는 데이터의 본질적 특징(잠재적 특성)을 압축적으로 표현한 저차원 벡터입니다.핵심 특징 목적: 데이터의 본질적/잠재적 특징 추출 및 압축 조건: 반드시 차원 축소(압축)를 전제로 함 의미 공간: 의미적 관계가 구조화된 공간을 형성주요 생성 방법 오토인코더(Autoencoder): 인코더-디코더 구조의 중간 벡터 VAE(Variational Autoencoder): 확률적 잠재 변수 GAN(Generative Adversarial Network): 생성 모델의 잠재 공간 딥러닝 모델의 중간층 출력: 모델이 추출한 추상적 특징4. 개념 간 관계Latent Vector와 Embedding Vector의 관계 포함 관계: Latent Vector ⊂ Embedding Vector 모든 Latent Vector는 Embedding Vector이지만 모든 Embedding Vector가 Latent Vector는 아님 차이점: Embedding Vector: 차원 확장/축소 모두 가능 Latent Vector: 반드시 데이터의 본질적 특징을 압축한 저차원 벡터 구분 예시 Embedding Vector이지만 Latent Vector가 아닌 경우: 10개 카테고리를 64차원 벡터로 변환 (차원 확장) 원-핫 인코딩보다 더 큰 차원의 임베딩 Latent Vector의 예: 오토인코더의 bottleneck 벡터 (1024차원 → 128차원) BERT의 [CLS] 토큰 벡터 (문맥 정보 압축) 5. 실제 적용 사례자연어 처리 Word Embedding: 단어를 300차원 벡터로 변환 (Embedding Vector) 문장 임베딩: 문장 전체를 768차원으로 압축 (Latent Vector)이미지 처리 CNN 특징 맵: 이미지의 의미적 특징 추출 (Latent Vector) 이미지 패치 임베딩: ViT에서 이미지 패치를 벡터화 (Embedding Vector)추천 시스템 사용자/아이템 임베딩: 사용자와 아이템을 벡터로 표현 (Embedding Vector) 협업 필터링 잠재 벡터: 사용자-아이템 상호작용의 잠재 패턴 추출 (Latent Vector)6. 결론 Embedding은 데이터를 벡터 공간에 매핑하는 과정 Embedding Vector는 임베딩 과정의 결과물로, 차원 확장/축소가 모두 가능 Latent Vector는 Embedding Vector의 특수한 경우로, 데이터를 어떠한 목표로 하는 본질적 특징으로 압축하여 잘 표현하는 저차원 벡터간간히 헷갈리는 개념이라 정리합니다." }, { "title": "깃허브 블로그 테마 변경!", "url": "/posts/220622-change-blog-theme/", "categories": "이야기", "tags": "", "date": "2022-06-23 22:00:00 +0900", "snippet": "어제(2022.06.22) 깃허브 블로그의 테마를 변경했다.Jekyll을 활용한 정적 블로그인 점은 그대로고, Minimal Mistakes에서 Chirpy로 테마만 변경했다.Minimal Mistakes는 별다른 고민 없이 구글에 깃허브 블로그 만드는 법해서 제일 위에 있던 테마로 설정했었다.그렇다 보니 테마가 그리 맘에 들진 않았었다. 서치에 투자...", "content": "어제(2022.06.22) 깃허브 블로그의 테마를 변경했다.Jekyll을 활용한 정적 블로그인 점은 그대로고, Minimal Mistakes에서 Chirpy로 테마만 변경했다.Minimal Mistakes는 별다른 고민 없이 구글에 깃허브 블로그 만드는 법해서 제일 위에 있던 테마로 설정했었다.그렇다 보니 테마가 그리 맘에 들진 않았었다. 서치에 투자한 시간과 만족도는 비례한다내 기준에서, Minimal Mistakes는 설정할 수 있는 부분이 꽤나 많은 느낌이라, 자유도가 높은 만큼 신경 쓸 것도 많아서 조금 불편했다.그래서 맘에 드는 테마 보이면 바꿔야지~~ 하면서 살다가 부캠의 한 캠퍼분 테마가 마음에 들어서 벤치마킹했다.작업 일정 자체를 최종 프로젝트가 끝나고, 1주일 정도 리드미와 리포트 작성, 지원서 작성 등으로 시간을 보내고 그 다음주에 하려 했어서 딱 목표 대로 완료했다 !대구 월광수변공원 근처 트릴커피에서 했다.호수뷰에서 코딩하면 항상 기분이 좋은 것 같다 ㅎ이제 글도 열심히 써야지~~잔디밭과 컨벤션이런 저런 시도들을 하다 보니 잔디밭이 엄청 진해졌다. 그리고 이런 개인 레포에는 컨벤션 같은 건 안 키우기로 했다.컨벤션이 협업할 때 유용한걸 체감하고 나니 오히려 개인적으로 하기엔 의미가 없다는게 느껴졌달까..협업할 때만 열심히 지키는걸로 하려고 한다 ^^ㅎ여담리액트 페이지도 관심이 있어서 마음에 드는 리액트 테마가 보이면 또 갈아탈 생각도 있다.그런데 리액트의 장점을 감안하더라도 지금 테마가 너무 마음에 들어서… 그럴 일이 있을지는 잘 모르겠다.그리고 지킬은 루비로 만들어져 있다 보니 bundle exec jekyll s로 루비 런타임을 계속 굴리면서 글을 쓰는데, 반응이 꽤나 빠릿빠릿하다.꽤나 매력이 있는 친구인 것 같다..! 평생 살다 보면 얘도 배워 볼 일이 있지 않을까?앞으로 블로그에 할 일 _posts폴더에 구분 없이 넣어 둔 글들 카테고리별로 정리하기 부캠 후기 및 근황 관련 글 작성 disqus를 추가할지는 아직 고민 중.. 저번에 넣었다가 광고를 너무 띄우길래 치웠었다. about 작성 쓰다 만 쿠다 관련 글 마저 정리 기술 관련된 내용들 정리해서 올리면서 공부하기(GIL, SOLID, 트랜스포머… 등등 생각 해 왔던 것들)" }, { "title": "Boostcamp AI Tech 3기 이미지 분류 대회 후기", "url": "/posts/boostcamp-ai-3rd-img-classification-competition/", "categories": "NAVER BoostCamp AI Tech", "tags": "NAVER BoostCamp AI Tech, 회고", "date": "2022-03-23 03:00:00 +0900", "snippet": " Repository Link대회 목표 모델의 성능 개선 효율 좋은 dataset, model, loss…etc 코드 작성고민해본 문제들 Task 분해에 대한 고민 해당 문제는 독립적으로는 18개의 class를 분류하는 문제이지만, Age, Gender, Mask 각각에 대한 분류 문제로 생각할 수 있다. 하지만, 독립적인 18개 분류 문제...", "content": " Repository Link대회 목표 모델의 성능 개선 효율 좋은 dataset, model, loss…etc 코드 작성고민해본 문제들 Task 분해에 대한 고민 해당 문제는 독립적으로는 18개의 class를 분류하는 문제이지만, Age, Gender, Mask 각각에 대한 분류 문제로 생각할 수 있다. 하지만, 독립적인 18개 분류 문제로 접근해도 그 수가 많지 않으므로 충분히 모델이 잘 분류할 것으로 기대할 수 있고 이에 따른 성능 비교가 필요하다. 또한, 한 개 모델이 3가지를 한번에 분류하는 경우와 각각의 모델이 Task에 대해 분류하는 경우에 대한 비교 역시 필요하다. 불균형 처리에 대한 고민 나이브하게 생각하였을 때, 불균형를 해결해 주는 방법을 통해 모델의 성능 개선을 꾀할 수 있을 것이다. 하지만, 가장 큰 불균형을 보인 데이터는 나이에 대한 데이터였으며, 나이에 대한 3개 클래스 중 2개 클래스는 비슷한 충분한 숫자를 가지고 있었다. 이에 따라 오히려 충분히 학습시키면, 오히려 나머지 2개 클래스의 디시전 바운더리가 더 명확해져 데이터가 부족한 클래스도 잘 예측할 수 있으리라 생각하였다. 데이터셋 일반화에 대한 고민 이미지 데이터이므로, 다양한 Augumentaion을 덧붙여서 데이터셋을 더 일반화 시킬 수 있다. 하지만 사람의 이미지이기에 뒤집어진 이미지가 들어올 리 없으므로, Vertical Flip이 필요하지 않은 등 Augmentation의 수준에 대하여 고민해 볼 필요가 있었다. custom loss에 대한 고민 제공받은 baseline에는 cross entropy 뿐 아니라 focal loss, f1 loss, label smoothing loss 등 다양한 custom loss가 정의되어 있었다. cross entropy loss를 줄이는 것은 정확도 개선에 확실히 도움이 되지만, 대회는 f1 점수를 이용해 채점하기에 이를 고려할 필요가 있었다. 또한, multi label classification 구현을 위해 전용 loss function을 구현할 필요가 있었다. pretrained model의 parameter freeze에 대한 고민 - 일반화 성능 향상에 효과적일까? 분류문제로 미리 훈련된 모델을 파인튜닝하여 활용하기에 쉽게 모델을 학습시킬 수 있다. 하지만 분류 레이어 뿐만 아니라 다른 학습된 부분까지 학습시키면 굉장히 빠르게 오버피팅될 수 있으며 이는 일반화 성능에 좋지 않은 효과를 가져다 줄 수 있다. 이를 위해 처음부터 분류 레이어가 아닌 부분에 대하여 고정시키거나, 일정한 수준의 학습이 이루어 진 후에 이를 고정시키고 분류 레이어만 학습시키는 등의 방법을 고민해볼 수 있다.해결하지 못한 문제 오버피팅에 대한 제어 Train Set과 Valid Set을 구분하여 학습시켰지만, 어느정도 학습된 이후 학습 정확도와 검증 정확도의 갭이 발생하며 오버피팅되는 모습을 관찰할 수 있기를 기대하였다. 하지만 오히려 학습을 시킬수록 검증 정확도까지 100%에 가까워지는 모습만 관찰할 수 있었고, 결국 오버피팅문제는 대회 내내 전혀 관리하지 못했다. 분류 레이어에 Dropout을 추가하는 정도로는 해결되지 않았고, 사람을 기준으로 Train과 Valid Set을 구분하였으니, 사람 자체에 대한 치팅은 일어날 수 없는 상황이었다. 해당 문제의 원인으로는 같은 배경에서 찍힌 비슷한 성별/연령의 사람들이 많았기 때문으로 추정하고 있으며, 사람이 아닌 배경을 보고 성별/연령을 구분하는 오류를 일으킨 것으로 생각한다. 이를 해결하기 위해 이미지의 중간을 잘라내서 학습하는 형태로 배경의 영향력을 줄여보려 시도하였지만, 큰 효과를 얻진 못했다.시도하지 못한 것 다른 목적의 모델을 파인튜닝하여 활용 예를 들어, yolo를 fine-tuning하여 대회에 적용해 보고 싶었다. 단순 호기심으로 진행 해보고 싶었던 아이디어였지만, 시간이 부족하여 진행하지 못했다. 별개로, 2개 이상의 모델을 묶어서 분류를 해보는 형태는 구현해 보았지만, 좋은 성능을 보이지는 않았다. 이미지 로딩 과정의 정교한 설계 이미지는 기본적으로 디스크에 저장되어 있으며, baseline에서는 디스크에서 그때 그때 읽어 feeding 시키는 형태로 구현되어 있었다. 이를 약간 개선하여 이미지를 메모리에 올린 후 feeding시키면 좀 더 빠르게 학습시킬 수 있을 것으로 생각하였지만, 디스크에서 그때 그때 읽어 feeding 시키는 것의 학습 시간 차이를 직접 느끼지 못했다. 하지만 이 과정에서 약간 안일하게 접근하여, 시간을 측정하는 코드를 추가하여 개선된 정도를 수치화하지 직접 이득을 비교해 보는 작업을 진행하지는 않았다. 테스트 코드 작성 Augmentation 코드를 작성하거나, dataset 관련 부분을 작성할 때, 테스트 코드를 작성하여 작성하면 개발 속도 및 좀 더 강건하고 효율적인 코드를 작성하는데 효과적으로 작용하였을 수 있을 것으로 생각되지만, 미처 작성하지 못했다.오피스아워 이후상위권 팀에서는 모두 Swin Transformer를 사용한 모습을 확인할 수 있었다. 해당 모델의 특징에 대해 공부해 보고 싶다.상위권의 두 팀은 background를 제거하는 방향으로 오버피팅 문제를 해결한 모습을 확인할 수 있었다. 다만 각기 segmentation과 detection의 다른 방법을 활용하였으며, 이 두가지 방법의 성능 차이를 눈으로 확인해 보아도 좋은 인사이트를 얻을 수 있을 것 같다.차후 목표사항dataset을 작성하는 과정에서, 처음에는 선형적으로 메모리에 이미지를 하나씩 올리는 형태로 구현하였었는데, 이 때 너무 오랜 시간이 필요로 했다. 결국은 None으로 이미지 리스트를 초기화한 후 dataloader의 worker가 첫 에포크를 돌 때 채워 넣도록 구현하였는데, 이 역시 정교한 설계로 생각되지는 않아 아쉽다. worker의 작동 원리 파악하고 GIL 등 파이썬의 특징에 대해 좀 더 공부하여 활용할 수 있도록 정리해보려 한다. 실험과 공부도 좋지만, 협업 과정에 있어 약간 아쉬운 부분이 있었다. 특히 일부 모델을 작성하는 과정에서 전용 loss가 필요했던 부분 등이 협업을 방해했던 부분이라 생각한다. 이를 더 쉽게 공유할 수 있도록 컨벤션 개선 및 쉽게 문서화할 수 있도록 노션 및 슬랙을 좀 더 적극적으로 활용할 필요가 있음을 느꼈다." }, { "title": "백준 1987 - 알파벳", "url": "/posts/boj-1987/", "categories": "problem solving, BOJ", "tags": "ps, boj", "date": "2022-02-19 03:00:00 +0900", "snippet": " Problem Link실행시간 상위권에 위치하면서 백준 알림 늘려주는 문제라서 한번 써 봅니다!가장 핵심 아이디어는 그래프 탐색을 통해 최대 칸 수를 세 보는 것이다.같은 알파벳은 두번 지날 수 없으므로, 최대 깊이는 26이다.깊이가 26이면 바로 탈출하도록 하고, 비트마스킹을 통해 이를 기록하면 빠르게 탐색할 수 있다.import sysR, C ...", "content": " Problem Link실행시간 상위권에 위치하면서 백준 알림 늘려주는 문제라서 한번 써 봅니다!가장 핵심 아이디어는 그래프 탐색을 통해 최대 칸 수를 세 보는 것이다.같은 알파벳은 두번 지날 수 없으므로, 최대 깊이는 26이다.깊이가 26이면 바로 탈출하도록 하고, 비트마스킹을 통해 이를 기록하면 빠르게 탐색할 수 있다.import sysR, C = map(int, sys.stdin.readline().split())board = [ list(sys.stdin.readline().strip()) for _ in range(R) ]visited = [ [0]*C for _ in range(R) ]max_depth = 0def loc(y, x): return 1&lt;&lt;(ord(board[y][x])-65)st = [ (0, 0, loc(0,0), 1) ]while st: y, x, mask, depth = st.pop() if depth &gt; max_depth: max_depth = depth if depth == 26: break for dy, dx in ( (1, 0), (-1, 0), (0, 1), (0, -1) ): ny, nx = y+dy, x+dx if 0 &lt;= ny &lt; R and 0 &lt;= nx &lt; C and not mask&amp;loc(ny, nx): if visited[ny][nx] ^ (mask|(loc(ny, nx))): visited[ny][nx] = (mask|(loc(ny, nx))) st.append( ( ny, nx, mask|(loc(ny, nx) ), depth+1 ) )" }, { "title": "Deep Learning Optimization", "url": "/posts/dl-optimization/", "categories": "NAVER BoostCamp AI Tech", "tags": "NAVER BoostCamp AI Tech, Deep Learning", "date": "2022-02-08 03:00:00 +0900", "snippet": "딥러닝에서의 최적화(Optimization)란 무엇일까?단순하게 생각하면랜덤으로 초기화된 모델의 weight들을Gradient Descent를 통해 cost를 최소화하는 방향으로 변화시키는 것 이라고 답변할 수 있을 것이다.하지만, 그 과정에서 고민하고 확인해야 할 것이 무엇이 있는지, 이를 위해 체크하면 좋을 것들에 대해 정리하고자 한다.일반화(Ge...", "content": "딥러닝에서의 최적화(Optimization)란 무엇일까?단순하게 생각하면랜덤으로 초기화된 모델의 weight들을Gradient Descent를 통해 cost를 최소화하는 방향으로 변화시키는 것 이라고 답변할 수 있을 것이다.하지만, 그 과정에서 고민하고 확인해야 할 것이 무엇이 있는지, 이를 위해 체크하면 좋을 것들에 대해 정리하고자 한다.일반화(Generalization)에 대하여최적화의 목표 중 하나는, 일반화 성능을 높이는 것이라 할 수 있다.일반화 성능이란 무엇인가? 일반화 성능을 높이면 무조건 좋은건가? 일반화란 어떤 의미일까?학습을 시키면 모델의 weight들이 training set의 cost를 최소화하는 방향으로 업데이트 된다.계속 학습을 시키게 되면 점점 더 잘 맞추게 된다.단, 일정 수준을 넘어버리면 training set에 과적합되어 test set에 대해선 잘 맞추지 못하는 모습을 보인다.reference. 모두에게 너무나 익숙한 그 그림학습 데이터에 너무 맞추는 것도 아니고, 너무 못 맞추는 것도 아니고.그 중간 어딘가가 가장 좋은(Balanced) 지점이라 할 수 있다.하지만 이는 너무 이론적인 말이다.실제 문제에서 항상 똑같이 적용할 수 있다고 할 수는 없다.실제 풀고자 하는 문제의 타겟은저 급격하게 변하는 모양새의 오버피팅된 모습일 수도 있고, 아예 다른 형태일 수도 있다.우리가 가정하고 있는 것은 데이터가 학습하고자 하는 어떤 목적에서 발생된 구조적인 형태를 가지고 있을 것이라고 기대 뿐이다.이 부분은 디테일한 분석을 통해 데이터에 대해 깊게 이해하고, 적절하게 문제를 정의하는 것이 가장 중요하다고 생각된다.일반화된 지점을 어떻게 확인할까?test error와 training error의 차이를 통해 얻는 일반화 갭의 상태를 보고 결정할 수 있다.단, 이를 통해서는 이 모델의 성능이 학습 성능이랑 비슷할 것이란 사실만 알 수 있다.학습 데이터에 대해 성능이 좋지 않으면(학습을 덜 했거나, 데이터에 노이즈가 너무 많이 껴 있거나, 데이터와 맞지 않는 모델을 사용했거나 등등..), 일반화 갭이 작다고 해서 잘 학습되었다고 할 수는 없다. 일반화 성능이 좋다 != 테스트 데이터 성능이 좋을 것이다 일반화 성능이 좋다 == 테스트 데이터 성능이 학습 데이터 성능과 비슷할 것이다모델의 성능에 대한 Bias와 Variance모델의 파라미터(weight, bias 등)를 말하는 것이 아님에 유의사격 시 탄착군과 비슷한 개념으로 생각할 수 있다.모델의 성능에 대한 Bias 비슷한 입력에 대해서 True Target에 얼마나 접근하는가. bias가 낮다 출력이 많이 분산 되더라도 평균적으로 True Target에 접근하는 경우 bias가 높다 True Target에 대해 평균적으로 많이 벗어나는 경우 모델의 성능에 대한 Variance 비슷한 입력을 넣었을 때 출력이 얼마나 일관적으로 나오는가 variance가 낮다 간단한 모델이 이런 경우가 많을 것이다. 비슷한 입력에 대해 둔감한 변화를 보인다. variance가 높다 비슷한 입력에 출력이 많이 달라진다. overfitting이 생길 가능성이 높아진다. Bias and Variance Trade-off 노이즈가 학습 데이터에 노이즈가 껴 있다고 가정했을 때, cost는 bias, variance, noise 3가지 파트로 이루어져 있다.데이터의 cost를 minimize하는 것은 사실 각각을 minimize 하는 것이 아니다.따라서, 하나가 줄어들면 하나가 커질 수 밖에 없고, 각 파트는 trade-off의 관계에 있다.이는 모델의 성능에 대한 이론적 한계(fundamental limit)가 된다.Gradient Descent에 대하여학습시 활용하는 데이터 수(배치 사이즈)에 따른 분류 Stocastic Gradient Descent 한 개 sample씩만 활용 Mini-batch Gradient Descent 일부 sample을 모아 data를 subset으로 만들어 활용 Batch Gradient Descent 전체 데이터를 한번에 활용 배치 사이즈를 얼마로 잡아야 할까?1개를 쓰면 너무 오래걸리고, 너무 많이 넣으면 GPU 메모리가 터지고,적절한 수를 찾아야 한다.논문: On Large-Batch Training for Deep Learning 라지 배치사이즈를 사용하면 sharp minimum에 도착한다. sharp minimum에서는 testing function에서 조금만 멀어져도 잘 동작하지 않을 수 있다.(위 이미지의 보라색 선 참고) flat minimum에 도착하면 testing function에서 조금 멀어져도 괜찮은 성능을 기대할 수 있다. flat minimum에 도착하면 일반화 성능이 높아진다. sharp minimum보다는 flat minimum에 도착하는 것이 더 좋다. 배치 사이즈를 작게 쓰는게 일반적으로 좋다.Gradient Desent Methods똑같이 Gradient Information만 이용해서어떻게 더 좋은 성능, 혹은 더 빠른 학습을 시킬 수 있을까? 에 대한 고민자동으로 미분을 해 주는 딥러닝 프레임워크의 핵심으로,Optimizer로 구현되어 있고 적절한 것을 골라 활용할 수 있다.각각이 왜 제안이 되었고, 어떤 성질이 있는지를 알아두면 좋다.reference (Stocastic) Gradient Descent 가장 기본적인 GD를 활용하는 방법. Gradient를 구해서 learning rate만큼 빼준다. lr을 적절히 잡아주는게 매우 어렵다. Momentum 관성 이전 배치에서 어느 방향으로 흘렀는지에 대한 정보를 활용하자. 한번 흘렀으면, 다음번에 조금 다르게 흘러도 이쪽 방향으로 흐르던 정보를 이어가자. momentum과 현재 Gradient를 합친 Accumulation Gradient를 사용 한번 흐른 Gradient를 유지시켜줘서 Gradient가 왔다갔다해도 잘 훈련되도록 도와준다. NAG(Nestrov Accelerated Gradient) Gradient를 계산할 때 Lookahead Gradient를 계산한다. 현재 자리에서 한번 가 보고 간 자리에서 계산한 것으로 Accumulation. 위의 방법들은 local minima에 왔다갔다 하며 닿지 못하는 모습을 보일 수 있는데, 이를 봉우리에 닿도록 도와줄 수 있다. Adagrad (Adaptive grad) 파라미터가 지금까지 얼마나 변해왔는지를 확인한다. 많이 변한 파라미터는 적게 변화시키고, 적게 변한 파라미터는 많이 변화시킨다. adaptive lr을 활용하게 된다. G가 계속 커지기 때문에 G가 무한대로 갈 수록 점차 학습이 멈추게 되는 문제가 있다. Adadelta G가 계속 커지는 현상을 막겠다. 타임스탬프 t를 윈도우 사이즈 만큼의 그래디언트 변화를 보겠다 이전 t개 동안의 G를 들고 있어야 된다. 파라미터가 커지면 힘들다. (파라미터 수 * t의 공간 필요) lr이 없다. RMSprop 논문을 통해 제안된 것은 아니고, Geoff Hinton이 강의에서 이러니까 잘 되더라 한게 레퍼런스(…) Adagrad에서 G를 구할 때 그냥 gradient square를 더하는 것이 아니라, exponential moving average를 더해 준다. stepsize(η)를 사용한다. Adam (Adaptive Moment Estimation) 일반적으로 가장 무난하게 사용. RMSprop을 함과 동시에, 모멘텀을 같이 활용 β1 : 모멘텀을 얼마나 유지시키는지 β2 : gradient squares에 대한 EMA 정보 η : learning rate ε : div by zero를 막기 위한 파라미터지만, 이 값을 잘 바꿔주는것도 실질적으론 중요하다 Regularization. 규제학습을 방해하는게 목적이다.학습을 방해함으로써 얻는 이점은학습 데이터에서 뿐만 아니라테스트 데이터에 대해서도 잘 동작하도록 하기 위함이다. Early stopping loss 상황을 계속 보면서 일찍 학습을 멈추자. 단, 학습을 멈출 때 test data를 활용하면 cheating이다. 보통 validation error를 이용. Parameter norm penalty 네트워크 파라미터가 너무 커지지 않도록 한다. 이왕이면 네트워크 weight가 작은 것이 좋다. 뉴럴넷이 만드는 함수의 공간(function space)을 최대한 부드러운 형태로 만들자. weight가 작으면 function space가 부드러워진다. 부드러운 함수일 수록 일반화 성능이 높을 것이다..! 라는 기대 Data augmentation 뉴럴넷에서 가장 중요한 것 중 하나. 데이터가 적으면, DL보다 일반적인 ML방법론이 더 좋다. 데이터가 많아지면, 많은 데이터에 대한 표현력이 ML방법론에선 부족하다. 따라서 DL방법론의 성능이 더 좋아진다. 단, 데이터를 변화시킴에도 정답이 변화하지 않는 수준에서 변화를 시킨다. Noise robustness 사실 왜 잘되는지 아직 의문이 있긴 하다…ㅋㅋ(완전히 해석되지 않았다) 입력 데이터에 noise를 넣는 것은 Data augmentation의 일부로 생각할 수도 있다. 학습시킬 때 노이즈를 웨이트에 넣어줘도(weight를 흔들어도) 좋을 수 있다… Label Smoothing 다른 Label의 샘플을 뽑아서 데이터와 라벨을 섞어준다. 왜 잘 될까,,,? 결국, 데이터들이 있는 공간 속에서 Decision Boundary를 찾는게 목표. 이 경계를 부드럽게 만들어 주는 효과. ex. mixup, cutmix CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features Dropout 뉴럴넷의 weight를 랜덤하게 0으로 바꿔준다. 각각의 뉴런들이 좀 더 robust한 feature를 잡을 수 있다라고 해석을 한다.. (수학적으로 증명된 것은 아님.) 서로 다른 N개의 신경망을 앙상블하는 형태라 표현하기도 한다. 일반적으로 쓰면 성능이 많이 올라가는 효과를 보인다. Batch normalization 논란이 참 많다..ㅋㅋ(완전히 해석되지 않았다) 내가 적용하고자 하는 BN 레이어의 통계량을 정규화. 레이어 단 입력의 각각의 값들에 대하여 평균이 0인 정규분포로 만들어버린다. 대부분의 경우 성능이 많이 올라간다… 성능을 올리는 것이 목표라면 활용하는게 좋다. 이 글에 매우 잘 정리되어 있다. " }, { "title": "Deep Learning 기본", "url": "/posts/dl-basics/", "categories": "NAVER BoostCamp AI Tech", "tags": "NAVER BoostCamp AI Tech, Deep Learning", "date": "2022-02-07 03:00:00 +0900", "snippet": "딥러닝의 중요 요소 학습을 시킬 데이터(Data) 학습할 모델(Model) 학습시키기 위한 손실 함수(Loss) 손실을 최소화하는 최적화 알고리즘(Optimization Algorithm)여러 차원의 벡터 공간과 선형 변환 세상이 선형으로만 이루어져있지 않다. 1차원에서 1차원으로 가는 변환만을 찾지 않는다. N차원에서 M차원으로 가는 모델...", "content": "딥러닝의 중요 요소 학습을 시킬 데이터(Data) 학습할 모델(Model) 학습시키기 위한 손실 함수(Loss) 손실을 최소화하는 최적화 알고리즘(Optimization Algorithm)여러 차원의 벡터 공간과 선형 변환 세상이 선형으로만 이루어져있지 않다. 1차원에서 1차원으로 가는 변환만을 찾지 않는다. N차원에서 M차원으로 가는 모델을 찾고 싶을 수 있다. 행렬을 이용해 N차원에서 M차원으로 가는 mapping을 정의(affine transform) y = WTx + b (W와 b는 행렬과 벡터) 행렬을 곱한다 == 두 vector space간의 변환 선형성을 가지는 변환이 있을 때, 그 변환은 항상 행렬로 표현된다 행렬을 찾겠다 == 두 다른 차원 사이에 선형 변환을 찾겠다x와 y. 다른 차원의 두 벡터를 W 행렬을 통해 변환시켰다.Activation Function의 사용 핵심은, 모델이 최대한 많은 상황을 표현하도록 하는 것이다 네트워크를 깊게(deep) 쌓으려면? 중간에 hidden vector들을 획득 후 목표 벡터를 획득하는 방법이 있다 단, 이는 단순히 행렬들의 곱(선형 결합)에 불과하다 선형 결합은, 아무리 깊어져도 한단짜리와 표현력이 같다 즉, 단순한 선형 결합(linear transform)으로는 한계를 가진다 mapping이 표현할 수 있는 표현력을 극대화하기 위해서는? 비선형 결합(nonlinear transform)을 활용한다 비선형 결합을 거친 후의 선형 변환을 n번 반복하여 더 많은 표현력을 얻을 수 있다 어떤 Activation Function이 제일 좋은가? 사실, 어떤게 제일 좋을 지는 모른다. 문제마다 다르고, 상황마다 다르다. 각 활성 함수의 특징을 이해하고 문제에 활용하는 것이 좋다. Loss Function에 대하여 입력이 주어졌을 때 출력값과, 목표의 차이를 정의 이를 최소화하는 것이 목표 이것이 항상 우리의 목적을 완성하지는 않는다 항상 도움이 되지는 않을 수도 있다. MSE가 선형 회귀에서 최적인가? 제곱이 아니라 절댓값이나 네제곱을 활용해도 목표를 0으로 만드는 것은 같다. 하지만, outlier가 있을 경우 큰 영향을 받게 된다. 이렇듯, 어떻게 활용하는가에 따라 성질이 조금 달라진다. cross entropy가 분류 문제 해결에 최적인가? 분류를 잘 하는 관점에서는 결국 다른 값들 대비 높기만 하면 되는데? 다만 이를 수학적으로 표현하기 까다로우니 활용하는 것일 뿐. 확실하지 않은 확률 정보(uncertanty)를 같이 찾고 싶을 때는? probalistic loss function을 활용하여 likelihood를 최대화 하는 방향으로.. 이게 왜 우리가 원하는 결과를 얻어낼 수 있는지? 에 대한 이야기를 할 수 있어야 한다 loss function이 어떤 의미를 가지고 있는지 생각하자" }, { "title": "데이터 시각화", "url": "/posts/data-visualization/", "categories": "NAVER BoostCamp AI Tech", "tags": "NAVER BoostCamp AI Tech, visualization", "date": "2022-02-04 03:00:00 +0900", "snippet": "시각화 고려사항 목적 : 시각화하는 이유는 무엇인가 독자 : 누구를 대상으로 하는가 데이터 : 어떤 데이터를 시각화하는가 스토리 : 어떤 흐름으로 인사이트를 전달하는가 방법 : 전달하고자 하는 내용에 맞는 방법인가 디자인 : UI면에서 만족스러운 디자인인가데이터셋의 종류 정형 데이터 테이블 형태로 제공 row가 1...", "content": "시각화 고려사항 목적 : 시각화하는 이유는 무엇인가 독자 : 누구를 대상으로 하는가 데이터 : 어떤 데이터를 시각화하는가 스토리 : 어떤 흐름으로 인사이트를 전달하는가 방법 : 전달하고자 하는 내용에 맞는 방법인가 디자인 : UI면에서 만족스러운 디자인인가데이터셋의 종류 정형 데이터 테이블 형태로 제공 row가 1개 item columns는 attribute(feature) 쉽게 시각화 가능(통계적 특성, 상관관계, 비교 등) 시계열 데이터 시간 흐름에 따른 데이터 기온, 주가 등 정형 데이터 음성 비디오 등 비정형 데이터 시간 흐름에 따른 추세(trend), 계절성(seasonality), 주기성(cycle) 등 고려 지리 데이터 지도 정보와 보고자 하는 정보간의 조화가 중요 거리, 경로, 분포 등 다양하게 활용 관계형 데이터 객체(Node)와 객체 간의 관계(Link)를 시각화 크기, 색, 수 등으로 객체와 관계의 가중치 표현 휴리스틱하게 노드 배치하기 계층적 데이터 포함 관계가 분명한 데이터 Tree, Treemap, Sunburst 등 데이터의 분류 수치형(Numerical) 연속형(Continuous) : 길이, 무게 등 이산형(Discrete) : 주사위, 눈금 등 범주형(Categorical) 명목형(Norminal): 혈액형, 종교 등 순서형(Ordinal) : 학년, 별점 등 시각화 이해하기 마크와 채널 Mark : 점(Point), 선(Line), 면(Area)으로 이루어진 데이터 시각화 Channel : 각 마크를 변경할 수 있는 요소들 Position, Color, Shape, Tilt, Size 전주의적 속성(Pre-attentive Attribute) 주의를 주지 않아도 인지하게 되는 요소 동시에 사용하면 인지하기 어려움 적절히 사용시, 시각적 분리 효과(visual popout) " }, { "title": "PyTorch 기본", "url": "/posts/pytorch-basics/", "categories": "NAVER BoostCamp AI Tech", "tags": "NAVER BoostCamp AI Tech, pytorch", "date": "2022-01-24 03:00:00 +0900", "snippet": "PyTorch 핵심 요소 Numpy : Numpy 구조를 가지는 Tensor 객체로 array 표현 AutoGrad : 자동미분을 지원하여 DL 연산을 지원 Function : 다양한 형태의 DL을 지원하는 함수와 모델을 지원함PyTorch Operations 벡터를 다루는 python 기반 연산들은 대부분 numpy 기반(+ AutoGrad)...", "content": "PyTorch 핵심 요소 Numpy : Numpy 구조를 가지는 Tensor 객체로 array 표현 AutoGrad : 자동미분을 지원하여 DL 연산을 지원 Function : 다양한 형태의 DL을 지원하는 함수와 모델을 지원함PyTorch Operations 벡터를 다루는 python 기반 연산들은 대부분 numpy 기반(+ AutoGrad) Tensor 클래스를 이용하며, numpy like operations 대부분 적용 가능주요 Tensor Handling Method view : tensor의 shape을 변환(numpy의 reshape. 단, 메모리 상에서 약간 다르게 동작함) squeeze : 차원의 개수가 1인 차원을 압축(삭제) unsqueeze : 차원의 개수가 1인 차원을 추가파이토치 프로젝트 템플릿 처음엔 대화식 개발 과정(쥬피터 등)이 유리하지만, 개발 용이성 확보 필요(관리 용이, 유지보수 향상 목적) OOP 기반으로 모듈을 만들어 프로젝트 템플릿화 일반적인 모듈 목록 실행 설정 데이터 모델 학습 로깅, 지표 저장소 유틸리티 etc… 필요에 따라 nn.Module 딥러닝을 구성하는 Layer의 base calss input, output, forward, backward, parameter 등 정의 class MyLiner(nn.Module): def __init__(self, in_features, out_features, bias=True): super().__init__() self.in_features = in_features self.out_features = out_features self.weights = nn.Parameter(torch.randn(in_features, out_features)) self.bias = nn.Parameter(torch.randn(out_features)) ... def forward(self, x : Tensor): return x @ self.weights + self.bias ... Dataset 데이터의 입력 형태 정의, 입력 방식 표준화 Image, Text, Audio 등 다양한 형식 정의 가능 init, len, getitem의 구현이 필수적 from torch.utils.data import Dataset class CustomDataset(Dataset): def __init__(self, text, labels): self.labels = labels self.data = text def __len__(self): return len(self.labels) def __getitem__(self, idx): label = self.labels[idx] text = self.data[idx] sample = {\"Text\": text, \"Class\": label} return sample DataLoader 데이터의 배치를 생성 학습 직전의 데이터 변환을 책임 Tensor로 변환 및 Batch 처리 병렬적인 데이터 전처리 코드의 고민 필요 text = ['Happy', 'Amazing', 'Sad', 'Unhapy', 'Glum'] labels = ['Positive', 'Positive', 'Negative', 'Negative', 'Negative'] MyDataset = CustomDataset(text, labels) MyDataLoader = DataLoader(MyDataset, batch_size=2, shuffle=True) for dataset in MyDataLoader: print(dataset) # {'Text': ['Glum', 'Unhapy'], 'Class': ['Negative', 'Negative']} # {'Text': ['Sad', 'Amazing'], 'Class': ['Negative', 'Positive']} # {'Text': ['Happy'], 'Class': ['Positive']} model.save() 학습의 결과를 저장 모델 architecture와 파라미터를 저장 # 모델의 파라미터만 저장 # 같은 모델의 형태에서 파라미터만 load torch.save(model.state_dict(), os.path.join(MODEL_PATH, \"model.pt\")) new_model = TheModelClass() new_model.load_state_dict(torch.load(os.path.join(MODEL_PATH, \"model.pt\"))) ################################### # 모델의 architecture와 함께 저장 # 모델의 architecture와 함께 load torch.save(model, os.path.join(MODEL_PATH, \"model.pt\")) model = torch.load(os.path.join(MODEL_PATH, \"model.pt\")) Transfer learning 다른 데이터셋으로 학습한 모델을 현재 데이터에 적용 현재의 DL에서는 가장 일반적인 학습 기법 backbone이 잘 학습된 모델에서 일부분만 변경하여 학습을 수행 pretrained model을 활용시 모델의 일부분을 frozen 시킴" }, { "title": "통계학 맛보기", "url": "/posts/statistics-preview/", "categories": "NAVER BoostCamp AI Tech", "tags": "NAVER BoostCamp AI Tech, statistics", "date": "2022-01-21 03:00:00 +0900", "snippet": "통계적 모델링 적절한 가정 위에서 확률본포를 추정(inference) 모집단의 분포를 정확하게 알아낼 수 없음 -&gt; 근사적으로 확률분포를 추정 데이터와 추정 방법의 불확실성을 고려하여 위험을 최소화 모수적(parametric) 방법론 : 데이터가 특정 확률분포를 따른다고 가정 후 분포를 결정하는 모수를 추정 비모수적(nonparametr...", "content": "통계적 모델링 적절한 가정 위에서 확률본포를 추정(inference) 모집단의 분포를 정확하게 알아낼 수 없음 -&gt; 근사적으로 확률분포를 추정 데이터와 추정 방법의 불확실성을 고려하여 위험을 최소화 모수적(parametric) 방법론 : 데이터가 특정 확률분포를 따른다고 가정 후 분포를 결정하는 모수를 추정 비모수적(nonparametric) 방법론 : 특정 확률분포를 가정하지 않고, 데이터에 따라 모델의 구조 및 개수가 유연하게 바뀜 가정을 하는지, 아닌지에 따라 부여됨확률분포 가정 데이터를 생성하는 원리를 가장 먼저 고려 데이터 상태 분포 예시 0 또는 1 베르누이 분포 n개의 이산적인 값 카테고리 분포 [0,1] 베타 분포 0 이상의 값 감마분포, 로그정규분포 등 R(실수) 전체의 값 정규분포, 라플라스분포 등 모수 추정 표본평균, 표본표준편차(n-1) 등 활용 표집분포(sampling distribution) != 표본분포(sample distrubution) 표본평균의 확률분포(통계량의 확률분포) 정규분포를 따르지만, 포본의 확률분포는 정규분포를 따르지 않을 수 있다.최대가능도 추정법 모수를 추정하는 적절한 통계량은 조금씩 달라짐(표본평균, 표본분산만 활용하는 것은 위험) 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 maximum likelihood estimation, MLE 가능도(likelihood) 함수 : 모수를 따르는 분포Θ 가 x를 관찰할 가능성(확률이라기 보단, Θ에 대한 대소비교를 위함) 데이터가 독립적으로 추출되었을 경우, 확률밀도함수는 확률질량함수의 곱으로 표현 가능 로그함수의 성질을 이용하여, 곱셈을 덧셈으로 최적화 가능 정확도 관점에서 컴퓨터로 연산 가능 연산량을 O(n2) 에서 O(n)으로 줄여줌 손실을 최소화하는 방향으로 경사하강법을 사용하므로, 음의 로그가능도(negative log-likehood)를 최적화 MLE는 불편추정량을 보장하지 않음.딥러닝에서 최대가능도 추정법 소프트맥스 벡터는 카테고리분포의 모수를 모델링 정답레이블 y를 관찰데이터로 이용 확률분포인 소프트맥스 벡터의 로그가능도를 최적화KL 발산 두 확률분포 사이의 거리를 계산할 때 이용 최대가능도 추정법은 KL 발산을 최소화하는 것과 같다." }, { "title": "대구 빅데이터 분석 경진대회 3회 후기", "url": "/posts/daegu-bigdata-competition-2021/", "categories": "이야기", "tags": "회고", "date": "2021-11-01 03:00:00 +0900", "snippet": " 제3회 대구 빅데이터 분석 경진대회Repository Link어떤 대회인가?대구광역시에서 진행하는 빅데이터 분석 경진대회이다.올해로 3회째 진행하였으며, 점점 경진대회의 규모가 커지고 있는 느낌이다.대구광역시와 대구은행이 주최하기 때문인지 공공 또는 금융 두 부문 중 하나를 선택하여 참여할 수 있다.금융분야는 금융과 관련된 주제를 진행해야 하며, ...", "content": " 제3회 대구 빅데이터 분석 경진대회Repository Link어떤 대회인가?대구광역시에서 진행하는 빅데이터 분석 경진대회이다.올해로 3회째 진행하였으며, 점점 경진대회의 규모가 커지고 있는 느낌이다.대구광역시와 대구은행이 주최하기 때문인지 공공 또는 금융 두 부문 중 하나를 선택하여 참여할 수 있다.금융분야는 금융과 관련된 주제를 진행해야 하며, 대구은행 고객데이터 를 필수로 활용해야 한다.그리고 우리 팀은 금융분야로 참가하였다.일정 내용 날짜 장소 분석 계획서 마감 2021.06.04 온라인 접수 서류 통과자 발표(3배수) 및 분석 계획 발표 준비 - - 분석 계획 발표 2021.07.27 SW융합테크비즈센터 발표 통과자 발표(2배수) 및 분석 진행 - - 분석 결과 발표 2021.09.27 SW융합테크비즈센터 시상식 2021.10.14 대구은행 본점 대회의 일정은 위와 같았다.분석 내용주제 요약(디자인 정말 이쁘다!!) 딥러닝을 통해 경제변수 변화에 따른 금융시장 참여자의 행동 변화를 예측·분석하여 초개인화된 금융상품과 서비스를 추천 및 개발할 수 있도록 한다우리가 분석 목적으로 적어 놓은 문장.거창하게 요약하긴 했지만, 실제로는 복잡하지 않다.핵심은 경제 변수(환율, 금리 등)에 민감한 사람을 라벨링 하고, 이를 예측하는 것. 두가지로 나누어 볼 수 있다.계획과 구현의 변경점실 데이터를 보기 전 분석 계획서가 통과되어야 실 데이터를 만질 수 있다.데이터 설명서만 보고 기대한 것과, 실 데이터 사이 간극이 존재하다 보니 방법에서 약간의 변경점이 있었다. (실데이터 보기 전)분석 계획서 (실데이터 보기 전)분석 계획 발표자료 (분석 완료)분석 보고서 (분석 완료)분석 결과 발표자료방법을 약간 바꾼 가장 큰 이유는 데이터에 대한 우리 기대가 너무 컸다는 점이다.예로, 우리는 데이터 기간이 2018.01 ~ 2019.12이기에 매달의 존재할 것으로 기대했다.하지만 실제로는 단 3시점의 데이터만 있었기에 기준 시점으로 2달 전까지의 데이터를 생성하여 활용하였다.여기 표가 다 채워져 있기를 바랐었다.그래서 그랜저 인과관계 검정 대신 상관관계를 통해 민감 고객 분류를 시도한 점이 가장 큰 변경점이다.발표 중 받았던 일부 질문 train/test set을 다 합쳐서 7:3 하지 않고 왜 19,20/21로 나누었는가? 미래참조 문제 때문인데, 7:3으로 나누면 왜 안되는지를 위주로 답변하였다. 모델이 21년 샘플로 학습하고, 19년 샘플로 검증을 시도할 가능성이 생긴다. 이 경우, 19년 시점에선 절대 알 수 없는 데이터로 검증이 이루어진 셈이다. 위의 경우, 결과가 왜곡될 수 있으므로 피하는 것이 좋다. 예측하는데 있어 따로 pre trained model은 활용하지 않았는가? pre trained model. 딥러닝, 특히 자연어 처리와 이미지 처리에서 매우 중요한 개념이 아닐 수 없다. 근래 자연어 처리 프로젝트를 진행하며 BERT를 접해볼 기회가 있었다. 이에 깊게 파고든 것은 아니었지만, 써먹기 위해 대략적인 개념에 대해 찾아보았었다. 그리고 내가 이번 공모전을 준비하며 했던 생각은, BERT정도는 아니어도 대구은행 고객 데이터로 어느정도 유의미한 User Embedding을 만들어 활용할 수 있지 않을까? 하는 욕심이었다. 물론 위 질문에 대한 답변은 대구은행 고객데이터와 같은 형식의 input을 활용해 금융데이터를 학습시킨 pre trained 모델은 없기 때문에, 초기화된 모델의 weight들을 직접 학습시켰다고 답변했다. 내가 조금 더 공부하고, 시간이 넉넉하여 활용 가능성에 대한 검증까지 진행했다면, 내 욕심까지 덧붙여서 센스 있는 답변까지 가능했을지도 모르겠다. 하지만, 아직도 내 욕심에 대한 답변은 못 내겠다. 난 소프트웨어 전공이고, 아직은 겉핥기로 통계를 만져봤을 뿐이기에, 확신이 없는 것 같다. 사실 난 그저 개발이 재미있고, 모델링은 알고리즘만으로 해결하기 힘든 문제를 해결해 주기에 재미있게 느꼈던 것이 아닌가 싶은 생각도 한다. 개발을 좋아하지만, 빅 데이터 시대에 개발과 통계의 유착관계(?)는 점점 더 심해지지 않을까. 그렇다면 더 공부를 하면 이 욕심에 대한 내 스스로 마음에 드는 답변을 할 수 있지 않을까 기대해 본다. 시계열 데이터는 계절성을 가질 가능성이 높은데, 이는 어떻게 처리하였나? 위의 슬라이드와 어느정도 이어진다. 이에 대한 답변은 내가 19년 12월, 20년 12월 시점 데이터로 학습한 모델을 21년 4월 시점으로 검증하였으니 크게 문제되지 않을 것이다 라고 한 것을 같이 발표 갔던 친구가 받아서, input으로 해당 시점의 경제변수 시계열 데이터도 함께 넣어 주어 모델이 계절성에 대응할 수 있도록 했다고 정정해 주었다. 이 질문을 받고선, 내 통계 실력은 정말 겉핥기 수준밖에 안되는구나! 싶을 정도로 뒤통수를 쎄게 얻어맞은 듯한 느낌이었다. ‘계절성’ 이란 것을 알고는 있었는데, 내게 모델링을 할 때 있어 계절성은 전혀 고려하지 않은 개념이었기 때문이다. input으로 해당 시점의 경제변수 시계열 데이터도 함께 넣는 것이 내가 낸 아이디어이긴 했지만, 나는 input이 너무 부족한 상황에서 그 상황을 표현할 수 있는 정보를 더 추가하려는 취지였기 때문이다. 내게 있어 그건 그저 모델 성능을 높이기 위한 추가적인 데이터에 지나지 않았었다. 그래서 중요하지 않게 생각했고, 모델 구조 그림에서도 따로 표현하지 않고 생략으로 대체했다. 그저 LSTM에 다 때려넣는다고 답이 나오는게 아닌데.. 통계가 전공인 그 친구는 내 말을 듣고 계절성부터 떠올리지 않았었을까? 시계열 공부 할 필요성도 크게 느꼈고, 더 많은 경험이 필요하겠다 싶었다. 언제쯤 할 수 있을지는 잘 모르겠지만.. 내가 아쉬웠던 사항모델 활용 방안에서의 디테일우리의 주제는 초 개인화 서비스 제안으로 딱 박혀 있는데 정작 이 부분은 너무 부실하다.정교한 데이터 분석 및 예측은 앞단인 라벨링과 모델링에만 집중되어 있고, 활용인 제안부분은 그냥 모델이 예측한 것의 정성적인 활용 방안에 불과하다.약간의 변명을 해 보자면일단 앞단이 되어야 다음 단계가 진행이 되는데, 라벨링 과정에서도 데이터가 우리 예상과 달라 다른 해결 방안을 고안하느라 진행이 더뎠다.모델링에서도 input으로 이걸 활용해도 되는지, 논리적으로 문제는 없는지, 불균형한 데이터셋 처리와 같은 디테일에 대한 고민은 물론, 코딩하며 무수한 OOM과도 만나게 되다 보니 빠르게 진행하지 못했다.무엇보다 나는 그저 모델의 성능을 어떻게든 더 높일 방안만 생각하고 있었고 제안을 고민해야 한다는 사실은 전혀 내 머릿속에 없었다.이건 ‘딥러닝’을 활용해 보고 싶었던 내 욕심이 너무 반영되어 버린 결과가 아닌가 싶다.텐서플로우로 MLP만 만들어 보았었는데, CLOVA AI RUSH를 거치며 토치로 신경망을 자유롭게 만드는 방법을 막 인힌 상황에 몸이 달아서, 이걸로 신경망 모델링을 해보고 싶었던 욕심이 너무 컸던 것 같다.다시말해, ‘신경망 구조를 이렇게 하면 모델 성능이 개선 될 것이고 실제로 더 좋다!’ 같은 모델링 챌린지 마인드였다. 공모전의 목표를 보았어야 하는데..팀의 SW 전공자인 내가 이러고 있으니 진행이 잘 될리가 있나.. 이는 팀원들에게 매우 미안한 부분으로 남아있다.발표 자료 준비에서의 디테일분석 결과 발표 시 예시를 들 때 고용률을 활용하였다.고용률은 우리가 임의로 1달 lagging하여 사용하였다 보니, 분석 방법과 활용 방안에서의 이미지를 잘 보면 상관관계가 거꾸로 뒤집혀 있다.그리고 PPT에 이에 대한 설명이 아예 없었다.왜 이렇게 했나? 어쩌면 당연히 들어갔어야 할 부분인데 보고서랑 발표자료 후다닥 만들어 낸다고 미처 챙기지 못한 부분 같다.고용률에 대해서만 1달 lagging 한 이유는 정량적이라기 보단 정성적인 이유에 가깝다.정성적인 이유라지만 나름의 논리는 있었다.먼저, 우리의 원래 계획으로는 그랜저 인과관계 검정을 활용하는 것이었기에 정상 시계열임을 확인하기 전 일단 그랜저 인과관계 검정을 시도해 봤었다.그랜저 인과검정은 시차(lags)를 주어 두 시계열의 인과관계를 검정할 수 있으며, 적정한 시차를 찾아내는 것 역시 중요하다.그리고 이 시차는 과거 n달 전의 사건이 영향을 끼친다 라고 할 수 있다.우리는 가능한 여러 시차를 주어(많이는 못했다. 시계열이 짧았기 때문) 검정을 해 봤고, 고용률은 가장 설명력이 좋은(p-value가 가장 작은) 시차가 1달이었던 것으로 기억한다.이후 그랜저 인과검정의 선행요건이 정상 시계열을 만족해야 함을 확인하고, 우리의 대부분의 시계열이 이를 만족하지 않았기에 이 자료는 내가 따로 기록하지 않았다..또한, 논리를 생각해 봤을때, 고용률은 월급과 관계 있을 것 같다는 1차원적인 상상을 했는데, 그렇다면 한달 lagging 하는게 맞지 않나? 라는 생각을 했다.또한 상관관계도 한달 lagging한 상관관계는 설명이 너무 잘되는데 lagging 하기 전의 상관관계는 도무지 설명되지 않았던 이유도 있다.이런 나름의 논리는 있었지만 결국은 정성적이다 보니 어쩌면 무의식적으로, 정성적 기준을 도입했다는 이유 때문에 이 부분을 생각하는 것에 거부감이 들었을지도 모르겠다.이를 증명이라도 하듯 모든 것이 담겨 있어야 할 보고서에도 1달 시차 주었다 한줄만 달랑 들어가 있고 왜 그랬는지에 대한 이유는 아예 없다.다 잘해놓고..왜 그렇게 했나에 대한 집착은 또 하고 또 해도 부족하지 않나. 결국 이게 어떤 프로젝트를 진행해도 퀄리티(품질)의 핵심이 아닌가.그리고 이것을 다시 봐도 바로 내가 이해하도록, 다른 사람도 이해하도록 잘 만들어야 하는데, 이건 너무 큰 실책이었다.장려상에서 그친 것도 이 부분을 논리적으로 설명하지 못한 것이 너무 크지 않았나.. 나는 그렇게 생각이 든다.발표 중에서의 디테일이건 발표 태도의 문제에 가까운 것 같은데, 발표를 하면서 중간중간 청중(심사위원)분들과 아이컨택도 하며 반응 봐가며 진행을 했어야 하는데 그러질 못했다.일단 많은 내용을 다 담으려다 보니 열심히 말만 해야 겨우 10분 내외로 들어오는 분량의 스크립트였기에 이걸 어떻게든 10분 안에 다 말해야 한다는 압박이 있었던 것 같다.또한 이것은 어투의 문제에 가까운데, 질문에 답변할 때 너무 단호하게 대답하는 경향이 있었다..그냥 유들유들하게 대답하면 되는 문제인데 쓸데없이 각 세워서 전달하는 느낌.. 너무 긴장해서 그런건가.. 왜이러니.. 내가 말하면서도 깜짝 놀랐었다.그리고 위의 고용률 관련 질문을 받고는 가장 약한 부분을 받고 멘탈이 반쯤 깨져서 어버버버.. 하면서 다음 질문들 답변한 기분인데 같이 발표 갔던 친구가 잘 수습해 준 덕분에 잘 마무리 하고 나올 수 있었다.발표도 나에겐 아직 더 많은 경험이 필요할 듯 싶다..대회를 마치고가장 먼저, 같이 해준 팀원 친구들에게 너무 고맙다.‘딥러닝을 통해’역시 우리의 목표 중 하나였지만 저 상황에서 제일 성능 좋은 모델은 랜덤 포레스트였다.모델링 경험치도 부족하고, 데이터도 부족하고 하다 보니 발생한 상황인데 오히려 전화위복이라고 feature importance를 끌고 와서 활용 방안으로 넣어버렸다.다만 딥러닝 모델은 ‘시계열 길이를 늘리면 딥러닝이 더 성능이 좋아질 것이다’로 마무리하고 데이터가 부족해 증명하지 못한 것은 약간 아쉽다.어떤 프로젝트를 끝내고도 하는 생각이지만, 내가 좀 더 열심히 했으면.. 미리 비슷한 프로젝트를 해봤으면.. 발표 경험이 좀 있었으면.. 이번 프로젝트를 더 완벽하게 끝내지 않았을까 싶은 생각이 든다.있을 수 없는 일이지만 이런 아쉬운 생각은 항상 드는 듯 하다..그렇지만 이렇게 모두 다른 전공 셋이 모여 하나만 바라보고 달려서 혼자서는 절대 못해낼 프로젝트를 완료하고, 수상까지 했다는 데에서 너무 만족스럽다.또한 AI RUSH 이후로도 느꼈던 것이지만 모델링에 석사 이상을 선호하는 이유도 확실하게 느낄 수 있었다.아무래도 학사는 이런 일련의 과정들에 대해 경험치가 부족할 가능성이 높을 수 밖에 없지 않나..그래도 계속 공부해나가면 백엔드부터 모델링까지 아우를 수 있는 개발자가 될 수 있지 않을까?최종 결과는 장려상이다.우리 팀장님이 서울로 인턴을 가버려서 저 자리는 내가 갔다." }, { "title": "CUDA Architecture", "url": "/posts/cuda-architecture/", "categories": "Tech, CUDA", "tags": "CUDA", "date": "2021-10-24 03:00:00 +0900", "snippet": "CUDA hardware의 구조(Tesla GP100 예시) 1GPU에 6GPC(graphics processing cluster) 1GPC에 10Pascal SM -&gt; 1GPU에 60SM 1SM(unit) = 32SP + 16DP + 8SFU + 2Tex SP(streaming processor) : FP32 core, 메...", "content": "CUDA hardware의 구조(Tesla GP100 예시) 1GPU에 6GPC(graphics processing cluster) 1GPC에 10Pascal SM -&gt; 1GPU에 60SM 1SM(unit) = 32SP + 16DP + 8SFU + 2Tex SP(streaming processor) : FP32 core, 메인 CUDA core, ALU for a single CUDA thread DP(double precision) : FP64 core SFU(sepcial function unit) : sin, cos, square root 등 특별한 연산 1클락에 해결 가능 Tex(texture processor) : for graphics purpose, CUDA로 사용시 사용하지 않기도 하고 메모리로 쓰기도 함 CUDA 의 확장성 CUDA dedvice는 1~4개의 SM의 저가 모바일 기기부터 1000+의 고가 워크스테이션까지 매우 다양 thread block 개념을 도입하여 해결(SM 1개가 thread block 1개 처리) so, grid - block - thread의 계층 구조 필요 thread block 들이 SM에 자유롭게 assign 되어서 처리되는 구조 Each block can execute in any order relative to other blocksSM에서 CU(control Unit, SM당 1개)의 실행 구조 1개의 CU의 제어를 받아 32 core(SP) 가 물리적으로 동시에 실행 1개의 warp scheduler 32 thread가 같은 instruction을 동시 실행 SM 1개는 2048+ thread를 동시 관리 -&gt; memory의 느린 반응 속도 해결Thread와 Warp Thread는 독립적 실행 단위(실) Warp 평행하게 관리되는 여러개의 실(Warp를 만드는 것처럼 여러 실을 평행하게 관리) CUDA에서의 Warp는 32개의 thread(SM이 32개의 SP를 가지므로) lane: Warp 내에서의 thread의 index(0~31) block 에는 1024개의 thread가 있지만, 32개씩 끊어서 warp로 관리 20개 이상의 warp가 대기 상태로 있는 것이 효율적 memory access 시간을 고려 warp 전환간 거의 zero-overhead. 충분히 많은 register를 확보하고 있기 때문 warp scheduler는 HW로 구현되어 오버헤드 거의 없음 2레벨 병렬 처리 grid는 thread blocks로 이루어져 있으므로 SM에 병렬 처리 thread block은 여러 warp로 갈라져서 병렬 처리 warp / block 종료 시 다음 warp / block을 처리 자원 제약에 대한 고려가 필요하지만, thread수를 1024정도로 잡으면 문제없음 block의 실행 순서가 정해져 있지 않음warp id, lane id GPU assembly instruction으로 체크 가능 warp id : SM 내에서, 특정 warp의 ID number __device__ unsigned warp_id(void) { // this is not equal to threadIdx.x / 32 unsigned ret; asm volatile (\"mov.u32 %0, %warpid;\" : \"=r\"(ret)); return ret; } lane id : warp 내에서, 자신의 lane id __device__ unsigned lane_id(void) { unsigned ret; asm volatile (\"mov.u32 %0, %laneid;\" : \"=r\"(ret)); return ret; } " }, { "title": "CUDA Kernel Launch", "url": "/posts/cuda-kernel-launch/", "categories": "Tech, CUDA", "tags": "CUDA", "date": "2021-10-23 03:00:00 +0900", "snippet": "C/C++ function call syntaxvoid func_name( int param, … );for (int i = 0; i &lt; SIZE; ++i) { func_name( param, … );}CUDA kernel launch syntax__global__ void kernel_name( int param, … );kernel_na...", "content": "C/C++ function call syntaxvoid func_name( int param, … );for (int i = 0; i &lt; SIZE; ++i) { func_name( param, … );}CUDA kernel launch syntax__global__ void kernel_name( int param, … );kernel_name &lt;&lt;&lt; 1, SIZE &gt;&gt;&gt;( param, … );&lt;&lt;&lt; &gt;&gt;&gt; 는 쿠다 컴파일러가 책임진다(C, Cpp 문법에 존재하지 않는 연산자)CUDA 에서의 kernel launch many threads(ex. 1,000,000) on many core(ex. 1,000)가 일반적인 상황 쓰레드 관리를 위한 모델 계층구조 launches are hierarchical, (grid - block - thread) 커널이 grid를 만들어서 grid가 실행되는 구조 grid는 많은 block들을, block들은 많은 thread들을 가짐. thread가 묶여서 block, block이 묶여서 grid가 된다 thread 내부는 sequential execution. 프로그래머들이 sequential programming에 워낙 익숙하기 때문 but, 모든 thread는 병렬로 실행되므로 병렬처리의 이점을 누릴 수 있음 grid, block 구조는 최대 3차원 kernel_func&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(...); kernelFunc&lt;&lt;&lt;3, 4&gt;&gt;&gt;(...); kernelFunc&lt;&lt;&lt;dim(3), dim(4)&gt;&gt;&gt;(...); kernelFunc&lt;&lt;&lt;dim(3, 1, 1), dim(4, 1, 1)&gt;&gt;&gt;(...); " }, { "title": "첫 포스팅 테스트", "url": "/posts/first-post/", "categories": "이야기", "tags": "", "date": "2021-08-14 03:00:00 +0900", "snippet": "깃허브 블로그 테스트테스트입니다.22.06.22 업데이트", "content": "깃허브 블로그 테스트테스트입니다.22.06.22 업데이트" } ]
